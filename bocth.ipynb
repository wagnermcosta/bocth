{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wagnermcosta/bocth/blob/main/bocth.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1659c3a"
      },
      "source": [
        "## Estudo Bag of Concepts - Thesaurus\n",
        "\n",
        "> Bloco com recuo\n",
        "\n",
        "\n",
        "\n",
        "https://github.com/hank110/bagofconcepts  \n",
        "Wagner Miranda Costa dez/2023"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtU1-snnuwvi",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "!pip install rank_bm25\n",
        "!pip install bagofconcepts\n",
        "!pip install bottleneck"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ba5461cc"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M851bLcs-yht"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "#descomente linha abaixo para execução no Google Colab\n",
        "files_path = \"/content/drive/My Drive/dados/\"\n",
        "\n",
        "#descomente linha abaixo para execução local\n",
        "#files_path = \"./\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3f31f60f"
      },
      "outputs": [],
      "source": [
        "#configuraçao default para word2vec\n",
        "embedding_dim = 100\n",
        "context = 8\n",
        "min_freq = 30\n",
        "iterations = 5\n",
        "concepts = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pxhuH-62Bdi",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7dbc1ea"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas.core.frame import DataFrame\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import time\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "026e5838"
      },
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cBU2OUNK5iZ"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec, KeyedVectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJdy4yWXtobH"
      },
      "outputs": [],
      "source": [
        "# Import and download stopwords from NLTK.\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import download\n",
        "import nltk\n",
        "from gensim.similarities import SparseTermSimilarityMatrix, WordEmbeddingSimilarityIndex\n",
        "\n",
        "# Download stopwords list\n",
        "download('stopwords')\n",
        "stop_words = stopwords.words('portuguese')\n",
        "download('wordnet')\n",
        "download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0Tun0BazEfe"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "#lemmatizador em portugues\n",
        "#descomentar as linhas abaixo no Google Colab quando da primeira compilação (ou se der erro na carga da biblioteca pt_core_news_lg)\n",
        "import spacy.cli\n",
        "spacy.cli.download(\"pt_core_news_lg\")\n",
        "import pt_core_news_lg\n",
        "nlp = pt_core_news_lg.load()\n",
        "\n",
        "#descomentar linha abaixo no jupyter\n",
        "#nlp = spacy.load('pt_core_news_lg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6dbd28c"
      },
      "outputs": [],
      "source": [
        "#lemmatiza um texto\n",
        "#sentence: texto no formato string ou list\n",
        "#retorna: list de lemmas da sentence\n",
        "def lemma(sentence):\n",
        "    #verifica se argumento é list\n",
        "    if type(sentence) is list:\n",
        "        sentence = ' '.join(sentence)\n",
        "\n",
        "    doc = nlp(sentence)\n",
        "    #o \"if not ' ' in token.lemma_\" desconsidera lemma como 'em o' (na), 'por o' (pelo), etc.\n",
        "    return [token.lemma_.lower() for token in doc if (not ' ' in token.lemma_ and len(token.text) > 2) ]\n",
        "    #a linha abaixo deve ser descomentada, em substituição aa anterior, quando não se quiser aplicar lemmatização\n",
        "    #return [token.text.lower() for token in doc if (not ' ' in token.text and len(token.text) > 2) ]\n",
        "\n",
        "\n",
        "\n",
        "def isVerb(word):\n",
        "    token = nlp(word)[0]\n",
        "    return token.pos_ == \"VERB\"\n",
        "\n",
        "\n",
        "def isNoun(word):\n",
        "    token = nlp(word)[0]\n",
        "    return token.pos_ == \"NOUN\"\n",
        "\n",
        "def getPartOfSpeech(word):\n",
        "    token = nlp(word)[0]\n",
        "    return token.pos_\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5a5a9374"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "#monta lista de Vocabulario de Controle Externo (VCE)\n",
        "#corresponde à coluna termo da planilha de VCE, sem espaços em branco e outros caracteres\n",
        "\n",
        "from re import sub\n",
        "\n",
        "#converte para minúsculo, remove acentos e números\n",
        "def prepareString(termo):\n",
        "    termo = termo.replace('(',' ')\n",
        "    termo = termo.replace(')',' ')\n",
        "    termo = termo.replace('-',' ')\n",
        "    termo = termo.replace(',',' ')\n",
        "    termo = termo.replace('/',' ')\n",
        "    termo = termo.replace('.',' ')\n",
        "    termo = termo.replace(':',' ')\n",
        "    termo = termo.replace(';',' ')\n",
        "    termo = termo.replace('$','')\n",
        "    termo = sub(r'[0-9]', \" \", termo)\n",
        "    return termo.lower()\n",
        "\n",
        "\n",
        "#lematiza VCE\n",
        "thesaurus_words = []\n",
        "temp = pd.read_excel(files_path + 'vce.xlsx').termo.map(prepareString)\n",
        "for line in temp:\n",
        "    for word in line.split():\n",
        "        thesaurus_words.append(word)\n",
        "\n",
        "thesaurus_words = list(set(thesaurus_words))\n",
        "\n",
        "#thesaurus_words_lemma corresponde ao tesauro de controle externo\n",
        "thesaurus_words_lemma = lemma(thesaurus_words)\n",
        "thesaurus_words_lemma = list(set(thesaurus_words_lemma))\n",
        "\n",
        "translation = {39: None}\n",
        "thesaurus_words_lemma = [token for token in thesaurus_words_lemma]\n",
        "thesaurus_words_lemma.append('certame')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZClKZK43Ier"
      },
      "outputs": [],
      "source": [
        "#verificação da similaridade\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import TfidfModel\n",
        "from re import sub\n",
        "from gensim.utils import simple_preprocess\n",
        "from scipy.spatial.distance import cityblock\n",
        "\n",
        "\n",
        "\n",
        "#FUNÇÕES DE CÁLCULO DE SIMILARIDADE E DE PREPROCESSAMENTO\n",
        "\n",
        "\n",
        "#preprocessa um texto\n",
        "#argumento: texto no formato string\n",
        "#retorna: lista de tokens\n",
        "def preprocess(sentence):\n",
        "    # Tokenize, clean up input document string\n",
        "    sentence = sub(r'<img[^<>]+(>|$)', \" \", sentence)\n",
        "    sentence = sub(r'<[^<>]+(>|$)', \" \", sentence)\n",
        "    sentence = sub(r'\\[img_assist[^]]*?\\]', \" \", sentence)\n",
        "    sentence = sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', \" \", sentence)\n",
        "    sentence = prepareString(sentence)\n",
        "    #remove STOPWORDS (realizado preliminarmente para não comprementer lexico da lemmatização com artigos e preposições)\n",
        "    sentence = [token for token in sentence.lower().split() if token not in stop_words]\n",
        "\n",
        "    #verifica novamente não pertencer aos STOPWORDS\n",
        "    return [token for token in lemma(sentence) if token not in stop_words]\n",
        "\n",
        "\n",
        "#preprocessa um texto, desprezando palavras que não constem do Tesauro\n",
        "#argumento: texto no formato string\n",
        "#retorna: lista de tokens\n",
        "def preprocessOnlyThesaurus(sentence):\n",
        "\n",
        "    #considera apenas termos do thesaurus\n",
        "    return [token for token in preprocess(sentence) if (token in thesaurus_words_lemma)]\n",
        "\n",
        "\n",
        "\n",
        "#vetorizador usando preprocessamento\n",
        "vectorizer = CountVectorizer(tokenizer=preprocess)\n",
        "\n",
        "#vetorizador TF-IDF usando preprocessamento\n",
        "tfidfvec = TfidfVectorizer(tokenizer=preprocess)\n",
        "\n",
        "\n",
        "\n",
        "def similarity(model, text_sentence1, text_sentence2, w2v_sentence1, w2v_sentence2):\n",
        "\n",
        "    documents = [text_sentence1, text_sentence2]\n",
        "    print('documents', documents)\n",
        "\n",
        "    dictionary = Dictionary(documents)\n",
        "    print(dictionary)\n",
        "\n",
        "    termsim_index = WordEmbeddingSimilarityIndex(model)\n",
        "    termsim_matrix = SparseTermSimilarityMatrix(termsim_index, dictionary)\n",
        "    return termsim_matrix.inner_product(w2v_sentence1, w2v_sentence2, normalized=(True, True))\n",
        "\n",
        "\n",
        "\n",
        "#calculo de similaridade semantica usando Soft Cosine Measure (SCM)\n",
        "def similaritySCM(model, sentence1, sentence2):\n",
        "\n",
        "     documents = [sentence1, sentence2]\n",
        "     dictionary = Dictionary(documents)\n",
        "\n",
        "\n",
        "     #sentence1 = dictionary.doc2bow(sentence1)\n",
        "     #sentence2 = dictionary.doc2bow(sentence2)\n",
        "\n",
        "     documents = [sentence1, sentence2]\n",
        "     tfidf = TfidfModel(documents)\n",
        "\n",
        "     sentence1 = tfidf[sentence1]\n",
        "     sentence2 = tfidf[sentence2]\n",
        "\n",
        "     print('***')\n",
        "     print(sentence1)\n",
        "     print(sentence2)\n",
        "\n",
        "     termsim_index = WordEmbeddingSimilarityIndex(model)\n",
        "\n",
        "     termsim_matrix = SparseTermSimilarityMatrix(termsim_index, dictionary, tfidf)\n",
        "\n",
        "     return termsim_matrix.inner_product(sentence1, sentence2, normalized=(True, True))\n",
        "\n",
        "\n",
        "#calculo da distancia WMD\n",
        "def wmDistance(sentence1, sentence2):\n",
        "\n",
        "    return 0\n",
        "    # return model.wmdistance(sentence1, sentence2)\n",
        "\n",
        "\n",
        "#calculo similaridade bag of words\n",
        "def similarityBOW(sentence1, sentence2):\n",
        "\n",
        "    tfidf = tfidfvec.fit_transform([sentence1, sentence2])\n",
        "    return 1 - (((tfidf * tfidf.T).A)[0,1])\n",
        "\n",
        "#similaridade jaccard binária\n",
        "def jaccard_binary(x,y):\n",
        "    \"\"\"A function for finding the similarity between two binary vectors\"\"\"\n",
        "    intersection = np.logical_and(x, y)\n",
        "    union = np.logical_or(x, y)\n",
        "    similarity = intersection.sum() / float(union.sum())\n",
        "    return similarity\n",
        "\n",
        "#similaridade jaccard conjunto\n",
        "def jaccard_set(list1, list2):\n",
        "    \"\"\"Define Jaccard Similarity function for two sets\"\"\"\n",
        "    intersection = len(list(set(list1).intersection(list2)))\n",
        "    union = (len(list1) + len(list2)) - intersection\n",
        "    return float(intersection) / union\n",
        "\n",
        "\n",
        "\n",
        "#Similaridade cosseno\n",
        "#recebe como parametros os vetores de caracteristicas já normalizados\n",
        "#ou duas strings representando as sentenças a serem comparadas. Nesse caso, é calculado o TF-IDF das sentenças,\n",
        "#e os vetores resultantes são normalizados\n",
        "def cosineSimilarity (arg1, arg2):\n",
        "    '''\n",
        "    #verifica se parametros são strings (na verdade, verifica apenas o primeiro argumento!)\n",
        "    if isinstance(arg1, str):\n",
        "        #cria bow baseado em TF-IDF\n",
        "        bow = tfidfvec.fit_transform([arg1, arg2])\n",
        "        #bow = vectorizer.fit_transform([arg1, arg2])\n",
        "\n",
        "        #normaliza os vetores baseado na soma do proprio array\n",
        "        arg1 = bow.A[0]/np.sum(bow.A[0])\n",
        "        arg2 = bow.A[1]/np.sum(bow.A[1])\n",
        "    '''\n",
        "    return np.dot(arg1, arg2)/(np.linalg.norm(arg1)*np.linalg.norm(arg2))\n",
        "\n",
        "#Cosine distance\n",
        "# Equivale a 1 - similaridade por cosseno\n",
        "#recebe como parametros os vetores de caracteristicas já normalizados\n",
        "#ou duas strings representando as sentenças a serem comparadas. Nesse caso, é calculado o TF-IDF das sentenças,\n",
        "#e os vetores resultantes são normalizados\n",
        "def cosineDistance (arg1, arg2):\n",
        "\n",
        "    return 1 - cosineSimilarity(arg1, arg2)\n",
        "\n",
        "\n",
        "#calculo similaridade bag of concepts\n",
        "def euclideanDistance(sentence1, sentence2):\n",
        "    temp = sentence1 - sentence2\n",
        "    return np.sqrt(np.dot(temp.T, temp))\n",
        "\n",
        "\n",
        "\n",
        "#calculo da distância Manhattan (cityblock)\n",
        "def manhattanDistance (arg1, arg2):\n",
        "    return cityblock(arg1, arg2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def glaucoDistance(arg1, arg2):\n",
        "\n",
        "    x = np.copy(arg1)\n",
        "    y = np.copy(arg2)\n",
        "\n",
        "    #normaliza vetores\n",
        "    #a normalização deve ser a primeira operação, para considerar no max() os elementos que eventualmente\n",
        "    #serão \"zerados\" na operação seguinte\n",
        "    #x = x/[np.max(x)]\n",
        "    #y = y/[np.max(y)]\n",
        "\n",
        "    #\"zera\" elementos onde seu equivalente no outro vetor é igual a zero\n",
        "    #desta forma, a diferença elementWise dessas coordenadas será zero\n",
        "    x[np.where(y == 0)]  = 0\n",
        "    y[np.where(x == 0)]  = 0\n",
        "\n",
        "    #calcula manhattanDistance\n",
        "    return cityblock(x ,y)\n",
        "\n",
        "\n",
        "def glaucoDistanceOriginal(x,y):\n",
        "  soma = 0\n",
        "  refX = max(x)\n",
        "  refY = max(y)\n",
        "  for i in range(0, len(x)):\n",
        "      if(y[i] > 0 and x[i] > 0):\n",
        "          soma += abs( float((x[i]/refX)) - float((y[i]/refY)) )\n",
        "\n",
        "  return soma\n",
        "\n",
        "\n",
        "\n",
        "#Euclidean distance usando linalg.norm()\n",
        "#recebe como parametros os vetores de caracteristicas já normalizados\n",
        "#ou duas strings representando as sentenças a serem comparadas. Nesse caso, é calculado o TF-IDF das sentenças,\n",
        "#e os vetores resultantes são normalizados\n",
        "def euclidean (arg1, arg2):\n",
        "\n",
        "    #verifica se parametros são strings (na verdade, verifica apenas o primeiro argumento!)\n",
        "    if isinstance(arg1, str):\n",
        "        #cria bow baseado em TF-IDF\n",
        "        bow = tfidfvec.fit_transform([arg1, arg2])\n",
        "        #bow = vectorizer.fit_transform([arg1, arg2])\n",
        "\n",
        "        #normaliza os vetores baseado na soma do proprio array\n",
        "        arg1 = bow.A[0]/np.sum(bow.A[0])\n",
        "        arg2 = bow.A[1]/np.sum(bow.A[1])\n",
        "\n",
        "    #Euclidean distance usando linalg.norm()\n",
        "    return np.linalg.norm(arg1 - arg2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96d85c0e"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c6c6667"
      },
      "source": [
        "## Extendendo classe BOCModel (bag of concepts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ee544991"
      },
      "outputs": [],
      "source": [
        "import bagofconcepts as boc\n",
        "import pickle\n",
        "from collections import Counter\n",
        "from sklearn.utils.extmath import safe_sparse_dot\n",
        "import scipy.sparse\n",
        "from soyclustering import SphericalKMeans\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.sparse import csr_matrix\n",
        "import numpy as np\n",
        "#import umap\n",
        "\n",
        "# classe BOCModel2\n",
        "# inherits from BOCModel para permitir calculo de similaridade\n",
        "# com matriz bag of concepts gerada a partir do corpus de treinamento (calculos estao na classe BOCModel)\n",
        "class BOCModel2(boc.BOCModel):\n",
        "\n",
        "    #armazenara concepts-frequencies calculado em _apply_cfidf\n",
        "    #para poder ser reutilizado em _get_cfidf\n",
        "    cf = None\n",
        "    #cf de cada documento do corpus ponderado pela soma de cf do proprio documento\n",
        "    wcfdoc = None\n",
        "\n",
        "    #valores utilizados para geração do word2vec\n",
        "    num_embedding_dim = 0\n",
        "    num_context = 0\n",
        "\n",
        "    #classe metodo de clusterizacao utilizado\n",
        "    cluster_class = None\n",
        "\n",
        "    #culsterizacoes disponíveis\n",
        "    clusterMethodAllowed = ['km','skm']\n",
        "    #metodo utilizado para clusterizacao\n",
        "    clusterMethod = None\n",
        "\n",
        "    #indica se o modelo de clusterização será lido de arquivo gravado anteriormente\n",
        "    loadCluster = False;\n",
        "\n",
        "    #arquivo onde será armazenado o cluster\n",
        "    clusterFile = files_path + 'boc2cluster.pickle'\n",
        "\n",
        "    #construtor sobrescrito para receber valor para variáveis de geração do word2vec\n",
        "    #\n",
        "    #clusterMeth {'km', skm'}: tipo de clusterizaçao\n",
        "    def __init__(self, corpus, wv, idx2word, num_concept=100, iterations=5, random_state=42,\n",
        "                 num_embedding_dim=100, num_context=8, clusterMethod ='skm', loadCluster = False):\n",
        "        super().__init__(corpus, wv, idx2word, num_concept, iterations, random_state)\n",
        "\n",
        "        self.num_embedding_dim = num_embedding_dim\n",
        "        self.num_context = num_context\n",
        "        self.loadCluster = loadCluster\n",
        "\n",
        "        #verifica se tipo de clusterização é válida\n",
        "        if clusterMethod in self.clusterMethodAllowed:\n",
        "            self.clusterMethod = clusterMethod\n",
        "        else:\n",
        "            raise ValueError('Método de clusterização %s inválido. Métodos disponíveis: %s.' % (clusterMethod, self.clusterMethodAllowed))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #reaporoveita metodo BOCModel._create_bow(self):\n",
        "    #porem retornando o resultado ao inves de atribuir aa variavel da classe\n",
        "    def _get_bow(self, document, _idx2word=None):\n",
        "        rows=[]\n",
        "        cols=[]\n",
        "        vals=[]\n",
        "        word2idx={word:idx for idx, word in enumerate(self.idx2word)}\n",
        "\n",
        "        for i, doc in enumerate(document):\n",
        "            tokens_count=Counter([word2idx[token] for token in doc if token in word2idx])\n",
        "            for idx, count in tokens_count.items():\n",
        "                rows.append(i)\n",
        "                cols.append(idx)\n",
        "                vals.append(float(count))\n",
        "\n",
        "        return csr_matrix((vals, (rows, cols)), shape=(i+1, len(word2idx)))\n",
        "\n",
        "\n",
        "    # retorna vetor de características de um único documento, baseado\n",
        "    # no BagOfConcepts do corpus de treinamento\n",
        "    # document = documento pre-processado no formato list []\n",
        "    def getwcf(self, document, exibeMatrix = False):\n",
        "\n",
        "        #recupera w2v e indice, mas apenas o indice interessa\n",
        "        #_, _idx2word = boc.utils.train_gensim_w2v(corpus=document, embedding_dim=self.num_embedding_dim,\n",
        "        #                                          context=self.num_context, min_freq=1, iterations=self.iterations)\n",
        "        #cria BoW\n",
        "        #linha abaixo comentado pois o parâmetro _idx2word não é utilizado. Em seu lugar, o metodo\n",
        "        #self._get_bow() utiliza variavel de classe que contem este  mesmo conteudo\n",
        "        #_bow = self._get_bow(document, _idx2word)\n",
        "        _bow = self._get_bow(document)\n",
        "\n",
        "\n",
        "        csr_matrix = safe_sparse_dot(_bow, self.w2c)\n",
        "\n",
        "        if exibeMatrix:\n",
        "            print('csr_matrix', document, csr_matrix.get_shape(), csr_matrix.toarray() )\n",
        "\n",
        "        return csr_matrix.toarray()[0]\n",
        "\n",
        "    #reaproveita metodo BOCModel._apply_cfidf(self, csr_matrix):\n",
        "    #porem retornando o resultado ao inves de atribuir aa variavel da classe\n",
        "    def _get_cfidf(self, csr_matrix):\n",
        "\n",
        "        num_docs, num_concepts = csr_matrix.shape\n",
        "\n",
        "        trace('csr_matrix ')\n",
        "        trace(csr_matrix)\n",
        "        trace('num_docs ' + str(num_docs))\n",
        "        trace('num_concepts ' +str(num_concepts))\n",
        "\n",
        "        _, nz_concept_idx = csr_matrix.nonzero()\n",
        "\n",
        "        trace('len(nz_concept_idx): %d', str(len(nz_concept_idx)))\n",
        "        trace('nz_concept_idx ')\n",
        "        trace(nz_concept_idx)\n",
        "\n",
        "        cf = np.bincount(nz_concept_idx, minlength=num_concepts)\n",
        "\n",
        "        #combinacao com o cf da classe (baseado no corpus original)\n",
        "        newcf = np.multiply(cf, self.cf)\n",
        "\n",
        "\n",
        "        #codigo original comentado para utilizar\n",
        "        #a quantidade de documentos do corpus original\n",
        "        #icf = np.log(num_docs / cf)\n",
        "        icf = np.log(len(self.corpus) / newcf)\n",
        "\n",
        "\n",
        "        icf[np.isinf(icf)] = 0\n",
        "\n",
        "\n",
        "        trace('novo icf')\n",
        "        trace(icf)\n",
        "\n",
        "        trace('scipy.sparse.diags(icf)')\n",
        "        trace(scipy.sparse.diags(icf))\n",
        "\n",
        "        return safe_sparse_dot(csr_matrix, scipy.sparse.diags(icf))\n",
        "\n",
        "\n",
        "    #metodo reproduzido na subclasse para poder armazenar o concepts-frequencies (cf) como\n",
        "    #variavel de classe\n",
        "    def _apply_cfidf(self, csr_matrix):\n",
        "        num_docs, num_concepts = csr_matrix.shape\n",
        "        _, nz_concept_idx = csr_matrix.nonzero()\n",
        "\n",
        "        #codigos comentados para armazenar variavel de classe self.cf\n",
        "        #cf = np.bincount(nz_concept_idx, minlength=num_concepts)\n",
        "        #icf = np.log(num_docs / cf)\n",
        "        self.cf = np.bincount(nz_concept_idx, minlength=num_concepts)\n",
        "        icf = np.log(num_docs / self.cf)\n",
        "\n",
        "        #calcula cf ponderado para cada doc do corpus\n",
        "        #baseado no cf do próprio documento\n",
        "        self.wcfdoc = []\n",
        "        csr_matrix_temp = csr_matrix.toarray()\n",
        "        for i in range(num_docs):\n",
        "            self.wcfdoc.append(np.array(csr_matrix_temp[i]/np.sum(csr_matrix_temp[i])))\n",
        "\n",
        "        icf[np.isinf(icf)] = 0\n",
        "\n",
        "        self.boc = safe_sparse_dot(csr_matrix, scipy.sparse.diags(icf))\n",
        "\n",
        "    #retorna array de concepts frequency (ou relativo a um documento do corpus de treinamento, caso index informado)\n",
        "    def getwcfdoc(self, index=None):\n",
        "        if (index == None):\n",
        "            return self.boc.A\n",
        "        else:\n",
        "            return self.boc[index].A[0]\n",
        "\n",
        "\n",
        "    #metodo reproduzido na subclasse para armazenar skm (SphericalKMeans)\n",
        "    def _cluster_wv(self, wv, num_concept, max_iter=10):\n",
        "\n",
        "        #verifica se le cluster gerado anteriormente\n",
        "        if self.loadCluster:\n",
        "\n",
        "            #lê cluster gerado e gravado anteriormente\n",
        "            dbfile = open(self.clusterFile, 'rb')\n",
        "            self.cluster_class = pickle.load(dbfile)\n",
        "            dbfile.close()\n",
        "\n",
        "            #atribui \"labels_\" ao atributo self.wv_cluster_id\n",
        "            self.wv_cluster_id = self.cluster_class.labels_\n",
        "\n",
        "        else :\n",
        "\n",
        "            #não lê cluster gerado anteriormente: efetua nova clusterização\n",
        "            sM=scipy.sparse.csr_matrix(wv).astype('double')\n",
        "\n",
        "\n",
        "            #seleciona tipo de clusterizacao\n",
        "            if self.clusterMethod == 'skm':\n",
        "                self.cluster_class = SphericalKMeans(n_clusters=num_concept, max_iter=max_iter, verbose=0, init='similar_cut', sparsity='None')\n",
        "            elif self.clusterMethod == 'km':\n",
        "                self.cluster_class = KMeans(n_clusters=num_concept,  n_init=10, max_iter=max_iter, init='k-means++')\n",
        "            else:\n",
        "                raise Error('Método de clusterização inválido: %s. Válido: %s' % (self.clusterMethod,self.clusterMethodAllowed))\n",
        "\n",
        "\n",
        "            #clusteriza e grava resultado (\"labels_\") ao atributo self.wv_cluster_id\n",
        "            self.wv_cluster_id = self.cluster_class.fit_predict(sM)\n",
        "\n",
        "            #grava cluster para eventual leitura posterior\n",
        "            dbfile = open(self.clusterFile, 'ab')\n",
        "            # source, destination\n",
        "            pickle.dump(self.cluster_class, dbfile)\n",
        "            dbfile.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #retorna classe de clusterização\n",
        "    def getClusterClass(self):\n",
        "        return self.cluster_class\n",
        "\n",
        "    #retorna mapeamento word -> concept no formato Dictionary\n",
        "    def getDictWord2Concept(self):\n",
        "        return dict([wc_pair for wc_pair in zip(self.idx2word, self.wv_cluster_id)])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2642829c"
      },
      "outputs": [],
      "source": [
        "#classe para identificar (ou predizer) o conceito definido em um modelo bag-of-concepts (BOC)\n",
        "\n",
        "import scipy.spatial.distance as sdist\n",
        "import networkx as nx\n",
        "import bottleneck as bn\n",
        "\n",
        "\n",
        "# classe conceptCluster\n",
        "class conceptCluster:\n",
        "\n",
        "    #modelo que possui os clusters de conceitos\n",
        "    cluster_model = None\n",
        "\n",
        "    #word2vec que será utilizado nas predições\n",
        "    w2v = None\n",
        "\n",
        "    #dicionario que mapeia cada palavra ao seu conceito\n",
        "    word2concept = None\n",
        "\n",
        "    #grafo de relacionamento dos conceitos\n",
        "    conceptGraph = None\n",
        "\n",
        "    #distancia (no grafo) entre os conceitos\n",
        "    #a chave é a tupla (conceito1, conceito2)\n",
        "    conceptDistance = {}\n",
        "\n",
        "    #construtor\n",
        "    #cluster_model: modelo com os clusters de conceitos\n",
        "    #word2vec: word2vec utilizado para a geração dos conceitos\n",
        "    #word2concept: dicionario que mapeia cada palavra utilizada para geração do conceito com respectivo conceito\n",
        "    def __init__(self, cluster_model, word2vec, word2concept = None):\n",
        "        self.cluster_model = cluster_model\n",
        "        self.w2v = word2vec\n",
        "        self.word2concept = word2concept\n",
        "\n",
        "\n",
        "\n",
        "    #retorna menor distância entre dois conceitos, baseado no grafo de conceitos\n",
        "    #concept1, concept2: conceitos para calculo da menor distância no grafo de conceitos\n",
        "    #\n",
        "    #returns: menor distância entre os conceitos informados\n",
        "    def shortestPathDistance_concepts(self, concept1, concept2):\n",
        "\n",
        "        #recupera distância entre os conceitos, já calculada e armazenada anteriormente\n",
        "        #caso nao exista, calcula e armazena\n",
        "        try:\n",
        "            distance = self.conceptDistance[(concept1, concept2)]\n",
        "        except KeyError:\n",
        "            #distance = nx.shortest_path_length(self.conceptGraph, source=concept1, target=concept2)\n",
        "            distance = nx.dijkstra_path_length(self.conceptGraph, source=concept1, target=concept2)\n",
        "            #armazena distancia calculada\n",
        "            self.conceptDistance[(concept1, concept2)] = distance\n",
        "\n",
        "        return distance\n",
        "\n",
        "\n",
        "    #retorna menor distância entre o conceito de duas palavras, baseado no grafo de conceitos\n",
        "    #word1, word2: palavras para calculo da menor distância no grafo de conceitos\n",
        "    #\n",
        "    #returns: menor distância entre os conceitos informados\n",
        "    def shortestPathDistance_words(self, word1, word2):\n",
        "\n",
        "        try:\n",
        "            concept1 = self.word2concept[word1]\n",
        "            concept2 = self.word2concept[word2]\n",
        "\n",
        "            distance = self.shortestPathDistance_concepts(concept1, concept2)\n",
        "        except KeyError:\n",
        "            #palavra não existe\n",
        "            distance = -1\n",
        "\n",
        "        #print('shortestPathDistance word|concept ', word1, concept1, word2, concept2, distance)\n",
        "\n",
        "        return distance # self.shortestPathDistance_concepts(concept1, concept2)\n",
        "\n",
        "\n",
        "    #gera grafo de relacionamento entre os conceitos\n",
        "    #degree = quantidade de relacionamentos de cada conceito (grau/degree, na definição da teoria dos grafos). Default = 3\n",
        "    def generateConceptGraph(self, degree = 3):\n",
        "\n",
        "        self.conceptGraph = nx.Graph()\n",
        "\n",
        "        #recupera centroids do cluster de conceitos\n",
        "        centroids = self.cluster_model.cluster_centers_\n",
        "        #qtd_clusters = len(centroids)\n",
        "\n",
        "        #calcula as distancias entres todos os centroids\n",
        "        all_distances = sdist.cdist(centroids, centroids, metric='euclidean')\n",
        "\n",
        "        #para cada cluster , recupera as degree_th menores distancias aos demais (desprezando a distancia 0, que\n",
        "        #corresponde ao próprio), e adiciona ao grafo como aresta (edge). Como o menor invariavelmente será a distancia\n",
        "        #ao proprio cluster (distancia=0), o array closests_clusters_indexes terá um elemento a mais (degree+1). Este elemento\n",
        "        #será desconsiderado na inclusão do edge ao grafo\n",
        "        for id_cluster, cluster_distances in enumerate(all_distances):\n",
        "            closests_clusters_indexes = bn.argpartition(cluster_distances, degree)[:degree+1]\n",
        "\n",
        "            print\n",
        "\n",
        "            #adiciona os edges entre clusters ao grafo, com 'weight' = distância entre os nodes, exceto o que corresponde ao proprio cluster\n",
        "            edges = [(id_cluster, closest_cluster, {'weight': cluster_distances[closest_cluster]}) for closest_cluster in closests_clusters_indexes if closest_cluster != id_cluster]\n",
        "            self.conceptGraph.add_edges_from(edges)\n",
        "\n",
        "\n",
        "        return all_distances\n",
        "\n",
        "\n",
        "\n",
        "    #atualiza vocabulário do word2vec\n",
        "    #deve ser executado antes do primeiro predict() ou predictByCentroid()\n",
        "    def update_vocab(self, sentences):\n",
        "        self.w2v.min_count = 1\n",
        "        self.w2v.build_vocab(corpus_iterable=sentences, update=True)  # update = True ensures that words are added to vocab\n",
        "        self.w2v.train(corpus_iterable=sentences, epochs=iterations, total_examples=len(sentences))\n",
        "\n",
        "\n",
        "    #retorna o vector de características Word2vec da palavra informada\n",
        "    #word: palavra que se pretende o vetor\n",
        "    #\n",
        "    #returns: vetor word2vec\n",
        "    def getVector(self, word):\n",
        "        try:\n",
        "            return self.w2v.get_vector(word, norm=True)\n",
        "\n",
        "        except:\n",
        "            #erro caso a palavra não tenha sido considerada na geração original do word2vec\n",
        "            print('Palavra inexistente no vocabulário do word2vec: %s.' %word)\n",
        "            return np.zeros_like(self.w2v.vectors[0])\n",
        "\n",
        "\n",
        "    #retorna indice do cluster relativo a uma palavra\n",
        "    #word: palavra para identificar o conceito\n",
        "    #\n",
        "    #returns: índice do conceito\n",
        "    def predict(self, word):\n",
        "\n",
        "        #retorna predicto do vetor (palavra) informada\n",
        "        return self.cluster_model.predict([self.getVector(word)])[0]\n",
        "\n",
        "\n",
        "    #retorna indice do cluster relativo a uma palavra baseado na proximidade\n",
        "    #com o centroide de cada cluster\n",
        "    #word: palavra para identificar o conceito\n",
        "    #\n",
        "    #returns: índice do conceito\n",
        "    def predictByCentroid(self, word):\n",
        "\n",
        "        #verifica conceito da palavra a partir do dicionário recebido no construtor da classe\n",
        "        if (self.word2concept is not None):\n",
        "            try:\n",
        "                return self.word2concept[word]\n",
        "\n",
        "            #caso a palavra nao exista no mapeamento, ignora erro e prossegue metodo para calcular\n",
        "            except KeyError:\n",
        "                pass\n",
        "\n",
        "\n",
        "        #caso o mapeamento palavra-> conceito não houver sido informado,calcula\n",
        "\n",
        "        #recupera vetor da palavra\n",
        "        word_vector = self.getVector(word)\n",
        "\n",
        "        #centroides do modelo (s)k-means\n",
        "        centroids = self.cluster_model.cluster_centers_\n",
        "\n",
        "        #array, de tamanho len(centroids), das distancias da palavra aos centroides\n",
        "        distances = np.ones(len(centroids))\n",
        "\n",
        "        #calcula distância da palavra a cada centroide\n",
        "        for i in range(distances.size):\n",
        "            distances[i] = euclidean(word_vector, centroids[i])\n",
        "\n",
        "        #retorna indice da menor distância, o que corresponde ao cluster\n",
        "        return np.argmin(distances)\n",
        "\n",
        "\n",
        "    #retorna a distância de uma palavra ao centroide de um dado conceito\n",
        "    #word: palavra\n",
        "    #concept: conceito para calcular a distância da palavra ao seu centroid\n",
        "    #\n",
        "    #returns: distância da palavra ao centroid do conceito informado\n",
        "    def getDistanceConceptCentroid(self, word, concept):\n",
        "\n",
        "        #recupera vetor da palavra\n",
        "        word_vector = self.getVector(word)\n",
        "\n",
        "        #centroides do conceito informado\n",
        "        centroid = self.cluster_model.cluster_centers_[concept]\n",
        "\n",
        "        return euclidean(word_vector, centroid)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68c2706e"
      },
      "outputs": [],
      "source": [
        "# classe WeigthedVectors\n",
        "# gera a vetorização ponderada por meio de clusters de conceitos e do\n",
        "# word2vec do thesaurus\n",
        "class WeigthedVectors:\n",
        "\n",
        "    #cluster de conceitos (classe conceptCluster)\n",
        "    concepts = None\n",
        "\n",
        "    #word2vec do thesaurus\n",
        "    w2vth = None\n",
        "    index_to_key_thesaurus = None\n",
        "\n",
        "    #dictionary com a menor distancia de uma palavra a um item do thesaurus\n",
        "    word_distance_to_thesaurus = {}\n",
        "\n",
        "    #construtor\n",
        "    #concepts: cluster de conceitos (classe conceptCluster)\n",
        "    #word2vec_thesaurus: word2vec do thesauros\n",
        "    def __init__(self, concepts, word2vec_thesaurus, index_to_key_thesaurus = None):\n",
        "\n",
        "        self.concepts = concepts\n",
        "\n",
        "        self.w2vth = word2vec_thesaurus\n",
        "        self.index_to_key_thesaurus = index_to_key_thesaurus\n",
        "\n",
        "    #calcula o peso da palavra perante a lista do thesauros\n",
        "    #word: palavra a ter o peso calculado\n",
        "    #\n",
        "    #returns peso da palavra\n",
        "    def getWeightByWord(self, word):\n",
        "\n",
        "        #caso a palavra pertença ao tesauro, retorna 0\n",
        "        if word in self.index_to_key_thesaurus:\n",
        "            return 0\n",
        "\n",
        "        #palavra não pertence ao tesauro:\n",
        "\n",
        "        #se a menor distancia da palavra a um termo do tesauro já tiver sido calculada\n",
        "        #(armazenada no dictionary self.word_distance_to_thesaurus), retorna-a. Caso contrário, continua o cálculo\n",
        "        try:\n",
        "            return self.word_distance_to_thesaurus[word]\n",
        "        except KeyError:\n",
        "            pass\n",
        "\n",
        "\n",
        "        #ainda não foi calculada a menor distancia do palavra:\n",
        "        #calcula a distancia para cada palavra do tesauro (por meio do word2vec)\n",
        "        #armazena o valor calculado em self.word_distance_to_thesaurus\n",
        "        # e retorna o menor valor encontrado\n",
        "\n",
        "        #array, de tamanho len(self.wv2vth), das distancias da palavra a cada um dos vetores do tesauro\n",
        "        thesaurus_vectors = self.w2vth\n",
        "        distances = np.ones(len(thesaurus_vectors))\n",
        "\n",
        "        #recupera vetor da palavra\n",
        "        word_vector = self.concepts.getVector(word)\n",
        "\n",
        "        #calcula distância da palavra a cada termo do tesauro\n",
        "        for i in range(distances.size):\n",
        "            distances[i]  = euclidean(word_vector, thesaurus_vectors[i])\n",
        "\n",
        "        #obtem a menor distancia\n",
        "        #caso seja maior que 1, \"normaliza\", ou seja, define o valor como .99, uma vez que o peso\n",
        "        # é calculado como 1 - a distância (isso está sendo feito para nar ter peso \"zerado\")\n",
        "        menorDistancia = np.amin(distances)\n",
        "\n",
        "\n",
        "        #grava menor distancia da palavra a um termo do tesauro\n",
        "        self.word_distance_to_thesaurus[word] = menorDistancia\n",
        "\n",
        "\n",
        "        #retorna menor distancia da palavra a um termo do tesauro\n",
        "        return menorDistancia\n",
        "\n",
        "\n",
        "\n",
        "    #gera vetores ponderados a partir dos conceitos, de cada uma das sentenças\n",
        "    #sentences: sentencas que terão os vetores gerados\n",
        "    #\n",
        "    #returns lista de histogramas calculados a partir das matrizes ponderadas de cada sentença\n",
        "    def get_weighted_vectors(self, sentences):\n",
        "\n",
        "\n",
        "        #lista de palavras que existem no word2vec utilizado para gerar os clusters de conceitos\n",
        "        words_in_w2v = self.concepts.w2v.index_to_key\n",
        "\n",
        "        #array com os histogramas (H) ponderados de cada uma das sentenças\n",
        "        weighted_vectors = []\n",
        "\n",
        "        #quantidade de conceitos (# of clusters)\n",
        "        k = self.concepts.cluster_model.n_clusters\n",
        "\n",
        "        #para cada sentença, tokeniza e obtem o conceito (cluster) de cada palavra\n",
        "        #em seguida, calcula a ponderação p, que é distância da palavra di à sua palavra correspondente\n",
        "        #mais próxima no tesauro T (self.w2vth)\n",
        "        for i, sentence in enumerate(sentences):\n",
        "\n",
        "            print('%d sentenças processadas' % (i+1), end = '\\r')\n",
        "\n",
        "            #conceito (cluster) de cada palavra da sentença que pertença ao word2vec utilizado na geração\n",
        "            #dos clusters de conceitos\n",
        "            conceptByWord = [self.concepts.predictByCentroid(token) for token in sentence] # if token in words_in_w2v]\n",
        "\n",
        "            #calcula o peso para cada palavra da sentença que pertença ao word2vec utilizado na geração\n",
        "            #dos clusters de conceitos\n",
        "            weightByWord = [self.getWeightByWord(token) for token in sentence] # if token in words_in_w2v]\n",
        "\n",
        "            #matriz de ativação de cada palavra da sentença, com a dimensão n x k,\n",
        "            #onde n=qtde de palavras da sentença e k = qtde de conceitos (# of clusters) (definido fora do loop)\n",
        "            #matriz será inicializada com zero, pois a ativação utilizada será a hard,\n",
        "            #onde apenas a posição correspondente ao conceito terá o valor 1 atribuído\n",
        "            n = len(sentence)\n",
        "            mx_sentence = np.zeros([n,k])\n",
        "\n",
        "            #para cada palavra, aplica a função de ativação junto com a atribuição do\n",
        "            #respectivo peso\n",
        "            for word_index, concept_index in enumerate(conceptByWord):\n",
        "                #recupera peso da palavra\n",
        "                wordWeight  = (1 - weightByWord[word_index])\n",
        "                #ativa e aplica peso, simultaneamente\n",
        "                mx_sentence[word_index,concept_index] = wordWeight\n",
        "\n",
        "            #consolidação do histograma ponderado dos conceitos da sentença\n",
        "            temp = np.sum(mx_sentence, axis=0)\n",
        "            weighted_vectors.append(temp)\n",
        "\n",
        "\n",
        "        return weighted_vectors\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njXNURupLzn1"
      },
      "source": [
        "## Acórdão"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2x6ASscLUhm"
      },
      "outputs": [],
      "source": [
        "#recupera acordaos\n",
        "acordaos = pd.read_excel(files_path + 'ground_truth.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEUkEAleIhbL",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#preprocessa acordaos\n",
        "acordaos['sumariop'] = acordaos.sumario.map(preprocess)\n",
        "colunas = ['sumario','sumariop']\n",
        "acordaos = acordaos[colunas]\n",
        "acordaos.sumario = acordaos.sumario.apply(lambda x: x.lower().split())\n",
        "corpusAcordao = acordaos.sumariop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38c2e102"
      },
      "source": [
        "## Ground Truth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16560ec4"
      },
      "outputs": [],
      "source": [
        "#recupera ground truth\n",
        "ac_gt = pd.read_excel(files_path + 'ground_truth.xlsx')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBto72lD4fcv"
      },
      "source": [
        "### Enunciados do ground truth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6eMQFwp4fcv"
      },
      "outputs": [],
      "source": [
        "#monta lista com enuciados do ground truth\n",
        "dados = ac_gt.to_numpy()\n",
        "enunciados_gt = []\n",
        "\n",
        "#percorre ground truth, adicionado cada enunciado associado a um acordão a uma lista\n",
        "for index_acordao in ac_gt.index:\n",
        "    for index_enunciado in dados[index_acordao,3:]:\n",
        "        if (index_enunciado == 0):\n",
        "            break\n",
        "\n",
        "        enunciados_gt.append(index_enunciado)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b5feb8f"
      },
      "source": [
        "## Jurisprudência"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66bc1797",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "#recupera jurisprudencia\n",
        "juris_path = files_path + 'jurisvce.txt'\n",
        "\n",
        "#arquivo texto está no encoding ANSI (para windows)\n",
        "jurisOriginal = pd.read_table(juris_path, encoding = 'ISO-8859-1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0L56OxVknUGM"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "jurisOriginal['enunciadop'] = jurisOriginal.enunciado.map(preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4c0e4cb"
      },
      "outputs": [],
      "source": [
        "corpusJuris = jurisOriginal.enunciadop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cE2P1ayFBg8T"
      },
      "source": [
        "**Seleção de amostras dos enunciados de jurisprudência**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0ceb7e9"
      },
      "outputs": [],
      "source": [
        "#Seleciona as amostras, por meio de Amostragem Aleatória Simples (AAS) de enunciados de jurisprudência que serão utilizadas\n",
        "#na avaliação das técnicas.\n",
        "#A seleção deve ser feita após a atribuição do corpus, pois este deve ser integralmente utilizado para a vetorização\n",
        "\n",
        "import random\n",
        "\n",
        "QTD_AMOSTRAS = 385 # valor obtido com AAS\n",
        "QTD_AMOSTRAS_A_SELECIONAR = QTD_AMOSTRAS # - len(enunciados_gt)\n",
        "\n",
        "#retorna dataframe com amostra de enunciados de jurisprudência\n",
        "def seleciona_amostras_jurisprudencia():\n",
        "  lista_amostras = random.sample(range(0, len(jurisOriginal)), QTD_AMOSTRAS_A_SELECIONAR)\n",
        "  lista_amostras_a_selecionar = lista_amostras + enunciados_gt\n",
        "\n",
        "  jurisAmostra = jurisOriginal.copy() #.iloc[lista_amostras_a_selecionar]\n",
        "  jurisAmostra.to_excel(files_path + 'jurisamostra.xlsx', index=True)\n",
        "\n",
        "  return jurisAmostra\n",
        "\n",
        "\n",
        "juris = seleciona_amostras_jurisprudencia()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43016b3b"
      },
      "outputs": [],
      "source": [
        "# classe TimeCounter\n",
        "# registro do tempo de execução\n",
        "class TimeCounter:\n",
        "\n",
        "    #lista de registros de tempos\n",
        "    times = None\n",
        "\n",
        "    #construtor\n",
        "    def __init__(self):\n",
        "        self.times = {}\n",
        "\n",
        "    #inicia contador de tempo\n",
        "    #\n",
        "    #name: nome do contador\n",
        "    def start(self, name):\n",
        "        t = time.time()\n",
        "        self.times[name] = {'start': t}\n",
        "    #end start()\n",
        "\n",
        "\n",
        "    #finaliza contador de tempo\n",
        "    #\n",
        "    #name: nome do contador\n",
        "    def stop(self, name):\n",
        "        t = time.time()\n",
        "        self.times[name]['end'] = t\n",
        "    #end stop()\n",
        "\n",
        "\n",
        "    #imprime tempo total, em segundos, dos contadores\n",
        "    #\n",
        "    #name: nome do contador\n",
        "    def print(self, name):\n",
        "        print(name, ':', (self.getTime(name)))\n",
        "\n",
        "    #end print()\n",
        "\n",
        "\n",
        "    #retorna tempo de um contador (em segundos)\n",
        "    #\n",
        "    #name: nome do contador\n",
        "    def getTime(self, name):\n",
        "        return (self.times[name]['end'] - self.times[name]['start'])\n",
        "\n",
        "\n",
        "    #imprime todas os registros\n",
        "    #\n",
        "    def printAll(self):\n",
        "        for i in self.times.keys():\n",
        "            self.print(i)\n",
        "    #end printAll()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "207d47e2"
      },
      "source": [
        "## Cálculo de similaridade entre acórdãos selecionados e a base de jurisprudência\n",
        "### BoC-Thesaurus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9b7c5b67"
      },
      "outputs": [],
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "\n",
        "\n",
        "\n",
        "#cria matriz esparsa com valores bm25 a partir de matriz tfidf\n",
        "#tfidf_vector: vetor tfidf original (csr_matrix)\n",
        "#tfidf_vocab: vocabulário utilizado para a geração do tfidf\n",
        "#bm25: valores bm25 (BM25Okapi)\n",
        "#\n",
        "#returns bm25 convertido em matriz esparsa na dimensão do tfidf original (csr_matrix)\n",
        "def create_sparse_bm25(tfidf_vector, tfidf_vocab, bm25):\n",
        "\n",
        "    #numero de docs do tfidf\n",
        "    num_docs = tfidf_vector.shape[0]\n",
        "\n",
        "    #arrays para construção de csr_matrix do tfidf redimensionado\n",
        "    #sera construido a partir do formato de matriz COO: csr_array((data, (row_ind, col_ind)), [shape=(M, N)])\n",
        "    rows = [] # corresponde ao idx do documento\n",
        "    cols = [] # corresponde ao idx da palavra na matriz tfidf original\n",
        "    data = [] # valor do bm25\n",
        "\n",
        "    #recupera os valores tfidf das palavras de cada documento do vetor tfidf\n",
        "    for idx_doc in range(num_docs):\n",
        "        print(idx_doc, end = '\\r')\n",
        "\n",
        "        #doc (documento) corresponde a cada linha do vetor tfidf recuperado no formato de matriz de coordenadas,\n",
        "        #que retorna apenas as coordenadas com valores (palavras do vocabulario presentes neste documento),\n",
        "        #uma vez que há muita esparsidade na matriz tfidf original\n",
        "        doc = tfidf_vector[idx_doc].tocoo()\n",
        "\n",
        "        #recupera, do documento, os indices das palavras no vocabulario tfidf\n",
        "        #como esta sendo usado o formato de matriz em formato de coordenada, o atributo .col retorna as palavras\n",
        "        #do vocabulario tfidf existentes neste documento\n",
        "        words = doc.col\n",
        "\n",
        "        #para cada palavra do documento, recupera seu valor bm25\n",
        "        for idx_word in words:\n",
        "            word = tfidf_vocab[idx_word]\n",
        "            bm25_value = bm25.idf[word]\n",
        "\n",
        "            #cria coordenada, indexando por [idx_doc, idx_word] = bm25_value\n",
        "            try:\n",
        "                rows.append(int(idx_doc))\n",
        "                cols.append(int(idx_word))\n",
        "                data.append(bm25_value)\n",
        "            except:\n",
        "                print('****** ERRO ******')\n",
        "                print(idx_doc, word, bm25_value)\n",
        "                raise\n",
        "\n",
        "        # end for idx_word in words...\n",
        "\n",
        "    #end for idx_doc in range(num_docs)...\n",
        "\n",
        "    return csr_matrix((data, (rows, cols)), shape=tfidf_vector.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#reduz a dimensionalidade do vertor TF-IDF a partir dos conceitos obtidos n BOC-th\n",
        "#tfidf_vector: vetor tfidf original (csr_matrix)\n",
        "#tfidf_vocab: vocabulário utilizado para a geração do tfidf. É o mesmo da clusterização BOC-th\n",
        "#num_concepts: número de conceitos\n",
        "#word2concept: Dictionary com o mapeamento word -> concept\n",
        "#\n",
        "#returns tfidf redimensionado para shape(num_docs, num_concepts) (csr_matrix)\n",
        "def redim_tfidf_by_concept(tfidf_vector, tfidf_vocab, num_concepts, word2concept):\n",
        "\n",
        "    #numero de docs do tfidf\n",
        "    num_docs = tfidf_vector.shape[0]\n",
        "\n",
        "    #arrays para construção de csr_matrix do tfidf redimensionado\n",
        "    #sera construido a partir do formato de matriz COO: csr_array((data, (row_ind, col_ind)), [shape=(M, N)])\n",
        "    rows = [] # corresponde ao idx do documento\n",
        "    cols = [] # corresponde ao idx do conceito da palavra no vocabulario tfidf\n",
        "    data = [] # valor do tfidf. na criaçao da csr_matriz, os indices duplicados terão os valores somados, que é a intenção\n",
        "\n",
        "    #recupera os valores tfidf das palavras de cada documento do vetor tfidf\n",
        "    for idx_doc in range(num_docs):\n",
        "        print(idx_doc, end = '\\r')\n",
        "\n",
        "        #doc (documento) corresponde a cada linha do vetor tfidf recuperado no formato de matriz de coordenadas,\n",
        "        #que retorna apenas as coordenadas com valores (palavras do vocabulario presentes neste documento),\n",
        "        #uma vez que há muita esparsidade na matriz tfidf original\n",
        "        doc = tfidf_vector[idx_doc].tocoo()\n",
        "\n",
        "        #recupera, do documento, os indices das palavras no vocabulario  tfidf\n",
        "        #como esta sendo usado o formato de matriz em formato de coordenada, o atributo .col retorna as palavras\n",
        "        #do vocabulario tfidf existentes neste documento\n",
        "        words = doc.col\n",
        "\n",
        "        #para cada palavra do documento, recupera seu conceito a partir da lista de features do vetor\n",
        "        #tfidf e do mapeamento palavraXconceito obtido do BOC_model\n",
        "        for idx_word in words:\n",
        "            word = tfidf_vocab[idx_word]\n",
        "            concept = word2concept.get(word)\n",
        "            tfidf = tfidf_vector[idx_doc, idx_word]\n",
        "\n",
        "            #cria coordenada, indexando por [idx_doc, concept] = tfidf\n",
        "            try:\n",
        "                rows.append(int(idx_doc))\n",
        "                cols.append(int(concept))\n",
        "                data.append(tfidf)\n",
        "            except:\n",
        "                print('****** ERRO ******')\n",
        "                print(idx_doc, word, concept, tfidf)\n",
        "                raise\n",
        "\n",
        "        # end for idx_word in words...\n",
        "\n",
        "    #end for idx_doc in range(num_docs)...\n",
        "\n",
        "    return csr_matrix((data, (rows, cols)), shape=(num_docs, num_concepts))\n",
        "\n",
        "\n",
        "def bocthSimilarity(arg1, arg2):\n",
        "\n",
        "    #copia arrays para poder manipula-los sem alterar o original (numpy.arrays são objetos mutaveis)\n",
        "    x = np.copy(arg1)\n",
        "    y = np.copy(arg2)\n",
        "\n",
        "    #\"zera\" elementos onde seu equivalente no outro vetor é igual a zero\n",
        "    #desta forma, a diferença elementWise dessas coordenadas será zero\n",
        "\n",
        "    x[np.where(y == 0)]  = 0\n",
        "    y[np.where(x == 0)]  = 0\n",
        "\n",
        "    #calcula similaridade\n",
        "    return cosineSimilarity(x ,y)\n",
        "\n",
        "\n",
        "#Cálculo de similaridade entre acordaos e jurisprudencia usando tecnica\n",
        "#BoC-Thesaurus (os vetores de caracteristicas consideram a frequência ponderada de conceitos)\n",
        "#\n",
        "#boc_model: modelo BOCModel2 utilizado\n",
        "#acordaos: Dataframe dos acordãos\n",
        "#juris: Dataframe da amostra de Jurisprudência\n",
        "#jurisOriginal: Dataframe da Jurisprudência original (completa)\n",
        "#wvec_acordaos: vetores w2v BOC-Thesaurus de acórdão\n",
        "#wvec_juris: vetores w2v BOC-Thesaurus de jurisprudência\n",
        "#vectorsTfIdf: vetorização tf-idf do corpus. Caso não seja informada, será calculada\n",
        "#vectorsTfIdf_bocth: redução da dimensionalidade a partir do BoC. Caso não seja informada, será calculada\n",
        "#\n",
        "#return dataframe com o resultados do cálculo das similaridades entre acordãos e jurisprudência\n",
        "def similarityBocThesaurosFull(boc_model, acordaos, juris, jurisOriginal, wvec_acordaos, wvec_juris, vectorsTfIdf=None, vectorsTfIdf_bocth=None):\n",
        "\n",
        "    #armazena o tempo de processamento de cada técnica\n",
        "    tbow = []\n",
        "    tboc = []\n",
        "    tbocth = []\n",
        "    ttfidf = []\n",
        "    tbm25 = []\n",
        "    ttfidfbocth = []\n",
        "\n",
        "    #dicionarios para armazenar os enunciados recuperados e calculados\n",
        "    enunciados_bow = {}\n",
        "    enunciados_tfidf = {}\n",
        "    enunciados_boc = {}\n",
        "    enunciados_bocth = {}\n",
        "    enunciados_tfidfbocth = {}\n",
        "\n",
        "    #classificacao BM25\n",
        "    t = time.time()\n",
        "    print('Gerando classificação BM25...', end = '\\r')\n",
        "    #classificador bm25 (parametro = dataseries corpusJuris transformado em list)\n",
        "    bm25 = BM25Okapi(jurisOriginal.enunciadop.tolist() + acordaos.sumariop.tolist())\n",
        "    print('Gerando classificação BM25... Concluído.', time.time()-t)\n",
        "\n",
        "\n",
        "    #vetorizacao tfidf caso não tenha recebido o parametro vectorsTfIdf\n",
        "    if (vectorsTfIdf is None):\n",
        "      #vetorizacao TF-IDF considerando todo o corpus (jurisprudencia + acordaos)\n",
        "      t = time.time()\n",
        "      print('Gerando vetorização TF-IDF...', end = '\\r')\n",
        "      vectorizerTfIdf = TfidfVectorizer(lowercase=True)\n",
        "      #vetorizacao com a base de jurisprudencia original para que qualquer enunciado existente na base de\n",
        "      #amostras de jurisprudencia seja localizada\n",
        "      vectorsTfIdf = vectorizerTfIdf.fit_transform(pd.concat([jurisOriginal.enunciadop, acordaos.sumariop], axis=0).astype(str))\n",
        "\n",
        "      #vocabulario tfidf\n",
        "      tfidf_vocab = vectorizerTfIdf.get_feature_names_out()\n",
        "      print('Gerando vetorização TF-IDF... Concluído.', time.time()-t)\n",
        "\n",
        "    #posicao de inicio dos acordaos (ground truth) da lista de vetores jurisprudencia + acordaos\n",
        "    offset_acordaos = len(juris)\n",
        "\n",
        "    #valores bm25 no formato csr_matrix\n",
        "    newbm25 = create_sparse_bm25(vectorsTfIdf, tfidf_vocab, bm25)\n",
        "\n",
        "    #redução de dimensionalidade tfidf-bocth caso não tenha recebido o parametro vectorsTfIdf_bocth\n",
        "    if (vectorsTfIdf_bocth is None):\n",
        "      #aplicando redução de dimensionalidade no vetor TF-IDF\n",
        "      t = time.time()\n",
        "      print('Redução de dimensionalidade no vetor TF-IDF a partir do BOC-Th...')\n",
        "\n",
        "      vectorsTfIdf_bocth = redim_tfidf_by_concept(newbm25, tfidf_vocab, boc_model.num_concept, boc_model.getDictWord2Concept())\n",
        "\n",
        "      print('Redução de dimensionalidade no vetor TF-IDF a partir do BOC-Th... Concluído.', time.time()-t)\n",
        "\n",
        "      print('vectorsTfIdf.shape', vectorsTfIdf.shape)\n",
        "      print('vectorsTfIdf_bocth.shape', vectorsTfIdf_bocth.shape)\n",
        "\n",
        "    #construção da classifcação bm25 apenas com o corpus da amostra\n",
        "    bm25 = BM25Okapi(juris.enunciadop.tolist())\n",
        "\n",
        "    #lista para armazenar o resultado dos calculos\n",
        "    resultados = []\n",
        "    #lista para armazenar os scores bm25\n",
        "    juris_scores_bm25 = []\n",
        "\n",
        "    #loop para repetições\n",
        "    for runs in range(1,2):\n",
        "\n",
        "        print('Execução', runs)\n",
        "        #limpa listas\n",
        "        resultados.clear()\n",
        "        juris_scores_bm25.clear()\n",
        "\n",
        "        #para cada acordao, percorre toda jurisprudencia, verificando a similaridade\n",
        "        for index_acordao, sumario, sumariop in acordaos.itertuples(name=None):\n",
        "\n",
        "            #if (index_acordao == 2):\n",
        "            #    break\n",
        "\n",
        "            #vetor tfidf do acordao\n",
        "            tfidf_acordao = vectorsTfIdf[offset_acordaos + index_acordao].A[0]\n",
        "\n",
        "            #vetor tfidf_bocth do acordao\n",
        "            tfidfbocth_acordao = vectorsTfIdf_bocth[offset_acordaos + index_acordao].A[0]\n",
        "\n",
        "\n",
        "            #vetor w2v do acordao\n",
        "            wvec_acordao = wvec_acordaos[index_acordao]\n",
        "            #wvec_acordao = wvec_acordao/np.sum(wvec_acordao)\n",
        "\n",
        "            #wcf do acordao, que será comparado com toda a jurisprudência\n",
        "            wcfAcordao = boc_model.getwcf([sumariop])\n",
        "\n",
        "            #bow do sumario (acordao)\n",
        "            bow_sumariop = boc_model._get_bow([sumariop]).A[0]\n",
        "\n",
        "            #verifica a similaridade para cada enunciado\n",
        "            for index_juris, enunciado, enunciadop in juris.itertuples(name=None):\n",
        "\n",
        "\n",
        "                if (index_juris % 777 == 0):\n",
        "                    print('Acórdão %d - %d enunciados processados...' % (index_acordao, index_juris+1), end = '\\r')\n",
        "\n",
        "                #calcula as similaridades\n",
        "\n",
        "                #bow\n",
        "                t = time.time()\n",
        "                #recupera vetor do enunciado armazenado anteriormente\n",
        "                #caso nao exista, calcula e armazena\n",
        "                try:\n",
        "                    bow_enunciadop = enunciados_bow[index_juris]\n",
        "                except KeyError:\n",
        "                    bow_enunciadop = boc_model.bow[index_juris].A[0] #recupera o bow do enunciado já calculado na BOCModel2\n",
        "                    enunciados_bow[index_juris] = bow_enunciadop\n",
        "\n",
        "                similarity_bow = cosineSimilarity(bow_sumariop, bow_enunciadop)\n",
        "\n",
        "                tbow.append(time.time()-t)\n",
        "\n",
        "\n",
        "\n",
        "                #tfidf\n",
        "                t=time.time()\n",
        "                #recupera vetor do enunciado armazenado anteriormente\n",
        "                #caso nao exista, calcula e armazena\n",
        "                try:\n",
        "                    tfidf_enunciado = enunciados_tfidf[index_juris]\n",
        "                except KeyError:\n",
        "                    tfidf_enunciado = vectorsTfIdf[index_juris].A[0]\n",
        "                    enunciados_tfidf[index_juris] = tfidf_enunciado\n",
        "\n",
        "                similarity_tfidf = cosineSimilarity(tfidf_acordao, tfidf_enunciado)\n",
        "                ttfidf.append(time.time()-t)\n",
        "\n",
        "\n",
        "                #boc\n",
        "                t=time.time()\n",
        "                #recupera vetor do enunciado armazenado anteriormente\n",
        "                #caso nao exista, calcula e armazena\n",
        "                try:\n",
        "                    wcfEnunciado = enunciados_boc[index_juris]\n",
        "                except KeyError:\n",
        "                    wcfEnunciado = boc_model.getwcfdoc(index_juris)\n",
        "                    enunciados_boc[index_juris] = wcfEnunciado\n",
        "\n",
        "                similarity_boc = cosineSimilarity(wcfAcordao, wcfEnunciado)\n",
        "                tboc.append(time.time()-t)\n",
        "\n",
        "                #bocth\n",
        "                t=time.time()\n",
        "                #recupera vetor do enunciado armazenado anteriormente\n",
        "                #caso nao exista, calcula e armazena\n",
        "                try:\n",
        "                    wvec_enunciado = enunciados_bocth[index_juris]\n",
        "                except KeyError:\n",
        "                    wvec_enunciado = wvec_juris[index_juris]\n",
        "                    enunciados_bocth[index_juris] = wvec_enunciado\n",
        "\n",
        "                similarity_bocth = cosineSimilarity(wvec_acordao, wvec_enunciado)\n",
        "\n",
        "                tbocth.append(time.time()-t)\n",
        "\n",
        "\n",
        "\n",
        "                #weigthedVectors\n",
        "                #tfidf-bocth\n",
        "                t=time.time()\n",
        "                #recupera vetor do enunciado armazenado anteriormente\n",
        "                #caso nao exista, calcula e armazena\n",
        "                try:\n",
        "                    tfidfbocth_enunciado = enunciados_tfidfbocth[index_juris]\n",
        "                except KeyError:\n",
        "                    tfidfbocth_enunciado = vectorsTfIdf_bocth[index_juris].A[0]\n",
        "                    enunciados_tfidfbocth[index_juris] = tfidfbocth_enunciado\n",
        "\n",
        "                similarity_tfidfbocth = cosineSimilarity(tfidfbocth_acordao, tfidfbocth_enunciado)\n",
        "                ttfidfbocth.append(time.time()-t)\n",
        "\n",
        "\n",
        "                #registra resultado\n",
        "                #linha abaixo comentada para 'economizar' uso de memoria\n",
        "                resultados.append([index_acordao, index_acordao, index_juris, index_juris,\n",
        "                                   similarity_bow, similarity_boc, similarity_bocth, similarity_tfidf, similarity_tfidfbocth])\n",
        "\n",
        "            #end for index_juris...\n",
        "\n",
        "            #bm25\n",
        "            t=time.time()\n",
        "            #calcula scores bm25 da query (sumariop) perante o corpus (jurisprudencia) e adiciona na lista\n",
        "            juris_scores_bm25.append(bm25.get_scores(sumariop))\n",
        "            tbm25.append(time.time()-t)\n",
        "\n",
        "\n",
        "            print('Acórdão %d - %d enunciados processados          ' % (index_acordao, index_juris+1), end = '\\r')\n",
        "\n",
        "        #end for index_acordao...\n",
        "\n",
        "        print()\n",
        "        print('Convertendo  array de scores bm25...', end = '\\r')\n",
        "        t=time.time()\n",
        "        #converte o array de scores para um array unidimensional com tamanho = #acordaos x #jurisprudencia\n",
        "        juris_scores_bm25 = [np.concatenate(juris_scores_bm25).tolist()]\n",
        "        juris_scores_bm25 = np.array(juris_scores_bm25).T\n",
        "        #aplica minmax-normalization nos scores para deixá-los na faixa [0,1]\n",
        "        juris_scores_bm25 = juris_scores_bm25/np.max(juris_scores_bm25)\n",
        "\n",
        "        #concatena aos resultados das mediçoes de cada metrica os scores bm25 (está separado pois o cálculo dos scores\n",
        "        #é feito de uma unica vez para cada acordao). Essa concatenação é feita adicionando uma coluna ao final\n",
        "        #do array 'resultados'. Por isso é necessário fazer o transpose (.T) do array juris_scores_bm25\n",
        "        resultados = np.concatenate((resultados, juris_scores_bm25), axis=1)\n",
        "        print('Convertendo  array de scores bm25... Concluído.', time.time()-t)\n",
        "\n",
        "\n",
        "    #end for runs...\n",
        "\n",
        "    print('..................................................')\n",
        "    print('Quantidade de execuções:', runs)\n",
        "    print('Tempo médio bow  :', np.mean(tbow), '±', np.std(tbow))\n",
        "    print('Tempo médio boc  :', np.mean(tboc), '±', np.std(tboc))\n",
        "    print('Tempo médio bocth:', np.mean(tbocth), '±', np.std(tbocth))\n",
        "    print('Tempo médio tfidf:',  np.mean(ttfidf), '±', np.std(ttfidf))\n",
        "    print('Tempo médio bm25 :',  np.mean(tbm25)/len(juris), '±', np.std(tbm25))\n",
        "    print('BM25 len', len(tbm25), '   sum', np.sum(tbm25))\n",
        "    print('Tempo médio tfidf-bocth :',  np.mean(ttfidfbocth), '±', np.std(ttfidfbocth))\n",
        "    print('..................................................')\n",
        "\n",
        "    return resultados\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b1c2ccf"
      },
      "source": [
        "### Avaliação a partir do ground truth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06a8e9ba"
      },
      "outputs": [],
      "source": [
        "#calculo do mAP a partir do ground truth\n",
        "\n",
        "\n",
        "def calcula_mAPFull(similaridades):\n",
        "\n",
        "    #groundTruth convertido em numpy.array\n",
        "    dados = ac_gt.to_numpy()\n",
        "\n",
        "    mapa = []\n",
        "    #lista de mAP bow e boc de cada  acordao\n",
        "    mAP = []\n",
        "    #para cada acordao, localiza a posicao de seus enunciados associados\n",
        "    #nas similaridades calculadas\n",
        "    for index_acordao in ac_gt.index:\n",
        "        mapa.append([index_acordao])\n",
        "\n",
        "        #filtra as similaridades por acordao e ordena por cada técnica de cálculo de similaridade\n",
        "        #a ordenacao bm25 é descendente, pois o valor é o score de similaridade do algoritmo OkapiBM25\n",
        "        #a ordenacao cgs é descendente, pois o valor é a similaridade\n",
        "        #os demais também, pois são relativos à cosine similarity [0,1]\n",
        "        sim_ac_bow = similaridades[similaridades.idx_sumario.eq(index_acordao)].sort_values(['bow'], ascending=[False], ignore_index=True)\n",
        "        sim_ac_boc = similaridades[similaridades.idx_sumario.eq(index_acordao)].sort_values(['boc'], ascending=[False], ignore_index=True)\n",
        "        sim_ac_bocth = similaridades[similaridades.idx_sumario.eq(index_acordao)].sort_values(['bocth'], ascending=[False], ignore_index=True)\n",
        "        sim_ac_tfidf = similaridades[similaridades.idx_sumario.eq(index_acordao)].sort_values(['tfidf'], ascending=[False], ignore_index=True)\n",
        "        sim_ac_tfidfbocth = similaridades[similaridades.idx_sumario.eq(index_acordao)].sort_values(['tfidfbocth'], ascending=[False], ignore_index=True)\n",
        "        sim_ac_bm25 = similaridades[similaridades.idx_sumario.eq(index_acordao)].sort_values(['bm25'], ascending=[False], ignore_index=True)\n",
        "\n",
        "        #verifica posicao da similaridade para cada enunciado\n",
        "        #dados[index_acordao,3:] = enunciados associados ao acordao (a partir da 4ª coluna)\n",
        "        for index_enunciado in dados[index_acordao,3:]:\n",
        "\n",
        "            if (index_enunciado == 0):\n",
        "                break\n",
        "\n",
        "            mapa[-1].append([index_enunciado])\n",
        "\n",
        "            #localiza posicao do enunciado nas similaridades do acordao\n",
        "            pos_bow = sim_ac_bow[sim_ac_bow.idx_enunciado.eq(index_enunciado)].index.values[0]\n",
        "            pos_boc = sim_ac_boc[sim_ac_boc.idx_enunciado.eq(index_enunciado)].index.values[0]\n",
        "            pos_bocth = sim_ac_bocth[sim_ac_bocth.idx_enunciado.eq(index_enunciado)].index.values[0]\n",
        "            pos_tfidf = sim_ac_tfidf[sim_ac_tfidf.idx_enunciado.eq(index_enunciado)].index.values[0]\n",
        "            pos_tfidfbocth = sim_ac_tfidfbocth[sim_ac_tfidfbocth.idx_enunciado.eq(index_enunciado)].index.values[0]\n",
        "            pos_bm25 = sim_ac_bm25[sim_ac_bm25.idx_enunciado.eq(index_enunciado)].index.values[0]\n",
        "\n",
        "            mapa[-1][-1].append(['bow',pos_bow])\n",
        "            mapa[-1][-1].append(['boc',pos_boc])\n",
        "            mapa[-1][-1].append(['bocth',pos_bocth])\n",
        "            mapa[-1][-1].append(['tfidf',pos_tfidf])\n",
        "            mapa[-1][-1].append(['tfidfbocth',pos_tfidfbocth])\n",
        "            mapa[-1][-1].append(['bm25',pos_bm25])\n",
        "\n",
        "        #end for index_enunciado...\n",
        "\n",
        "        #cálculo do mAP\n",
        "        #bow\n",
        "        ordered_Bow = sorted(mapa[-1][1:], key=lambda bow: bow[1][1])\n",
        "        arr_mAPBow = []\n",
        "        for i in range(len(ordered_Bow)):\n",
        "            valor = (i+1)/(ordered_Bow[i][1][1]+1)\n",
        "            arr_mAPBow.append(valor)\n",
        "\n",
        "        mAPBow = np.mean(arr_mAPBow)\n",
        "        stdBow  = np.std(arr_mAPBow)\n",
        "\n",
        "\n",
        "        #boc\n",
        "        ordered_Boc = sorted(mapa[-1][1:], key=lambda boc: boc[2][1])\n",
        "        arr_mAPBoc = []\n",
        "        for i in range(len(ordered_Boc)):\n",
        "            valor = (i+1)/(ordered_Boc[i][2][1]+1)\n",
        "            arr_mAPBoc.append(valor)\n",
        "\n",
        "        mAPBoc = np.mean(arr_mAPBoc)\n",
        "        stdBoc  = np.std(arr_mAPBoc)\n",
        "\n",
        "\n",
        "        #bocth\n",
        "        ordered_Bocth = sorted(mapa[-1][1:], key=lambda bocth: bocth[3][1])\n",
        "        arr_mAPBocth = []\n",
        "        for i in range(len(ordered_Bocth)):\n",
        "            valor = (i+1)/(ordered_Bocth[i][3][1]+1)\n",
        "            arr_mAPBocth.append(valor)\n",
        "\n",
        "        mAPBocth = np.mean(arr_mAPBocth)\n",
        "        stdBocth  = np.std(arr_mAPBocth)\n",
        "\n",
        "        #tfidf\n",
        "        ordered_Tfidf = sorted(mapa[-1][1:], key=lambda tfidf: tfidf[4][1])\n",
        "        arr_mAPTfidf = []\n",
        "        for i in range(len(ordered_Tfidf)):\n",
        "            valor = (i+1)/(ordered_Tfidf[i][4][1]+1)\n",
        "            arr_mAPTfidf.append(valor)\n",
        "\n",
        "        mAPTfidf = np.mean(arr_mAPTfidf)\n",
        "        stdTfidf  = np.std(arr_mAPTfidf)\n",
        "\n",
        "\n",
        "        #tfidfbocth\n",
        "        ordered_Tfidfbocth = sorted(mapa[-1][1:], key=lambda tfidfbocth: tfidfbocth[5][1])\n",
        "        arr_mAPTfidfbocth = []\n",
        "        for i in range(len(ordered_Tfidfbocth)):\n",
        "            valor = (i+1)/(ordered_Tfidfbocth[i][5][1]+1)\n",
        "            arr_mAPTfidfbocth.append(valor)\n",
        "\n",
        "        mAPTfidfbocth = np.mean(arr_mAPTfidfbocth)\n",
        "        stdTfidfbocth = np.std(arr_mAPTfidfbocth)\n",
        "\n",
        "\n",
        "        #bm25\n",
        "        ordered_Bm25 = sorted(mapa[-1][1:], key=lambda bm25: bm25[6][1])\n",
        "        arr_mAPBm25 = []\n",
        "        for i in range(len(ordered_Bm25)):\n",
        "            valor = (i+1)/(ordered_Bm25[i][6][1]+1)\n",
        "            arr_mAPBm25.append(valor)\n",
        "\n",
        "        mAPBm25 = np.mean(arr_mAPBm25)\n",
        "        stdBm25  = np.std(arr_mAPBm25)\n",
        "\n",
        "\n",
        "        #adiciona mAPs na lista de resultado\n",
        "        mAP.append([index_acordao, mAPBow, mAPBoc, mAPBocth, mAPTfidf, mAPTfidfbocth, mAPBm25])\n",
        "\n",
        "    #end for index_acordao...\n",
        "\n",
        "    #retorna dataframe\n",
        "    return DataFrame(data = mAP, columns=['idx_acordao', 'map_bow', 'map_boc', 'map_bocth', 'map_tfidf', 'map_tfidfbocth', 'map_bm25'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#calculo do precision@ a partir do ground truth\n",
        "\n",
        "#Calcula o recall@k das similaridades\n",
        "#O recall em k é a proporção de itens relevantes encontrados entres os top-k listados. Neste caso, relevante é o\n",
        "#enunciado que pertencem ao ground truth de determinado acórdão. Fórmula:\n",
        "#Recall@k = (# of recommended items @k that are relevant) / (total # of relevant items)\n",
        "#\n",
        "#k: posição objetivo na lista de similaridades para o cálculo\n",
        "#\n",
        "#returns\n",
        "def calcula_recallAtK(similaridades, k = 10):\n",
        "\n",
        "    #calcula o recall@k de um acordao especifico\n",
        "    #listagt: lista com as posições de cada técnica, na lista de similaridades,  dos enunciados do ground truth de determinado acórdão\n",
        "    #k: posição objetivo na lista de similaridades para o cálculo\n",
        "    #pos: posição, em cada item da lista 'listagt', da técnica que está se calculando o recall@k\n",
        "    def recallAtK(listagt, k, pos):\n",
        "\n",
        "      #a quantidade de itens relevantes corresponde ao tamanho de itens do ground truth\n",
        "      total_relevants =  len(listagt)\n",
        "      #contador da quantidade de itens relavantes em k\n",
        "      qty_relevants_at_k = 0\n",
        "\n",
        "      #percorre enunciados do gt, verificando a posição do cálculo de similaridade de determinada técnica (pos) na lista de similaridades\n",
        "      for item in listagt:\n",
        "        #posicao do cálculo de determinada técnica lista de similaridades (indicada pelo parâmetro 'pos'). Soma + 1 pois o índice começa em 0\n",
        "        position = item[pos][1]+1\n",
        "        #caso a posição na lista de similaridades esteja dentro do top-k, incrementa o contador\n",
        "        if position <= k:\n",
        "          qty_relevants_at_k = qty_relevants_at_k + 1\n",
        "\n",
        "      #recall@k\n",
        "      return qty_relevants_at_k/total_relevants\n",
        "\n",
        "\n",
        "    #groundTruth convertido em numpy.array\n",
        "    dados = ac_gt.to_numpy()\n",
        "\n",
        "    mapa = []\n",
        "    #lista de mAP bow e boc de cada  acordao\n",
        "    mRecallAtK = []\n",
        "    #para cada acordao, localiza a posicao de seus enunciados associados\n",
        "    #nas similaridades calculadas\n",
        "    for index_acordao in ac_gt.index:\n",
        "        mapa.append([index_acordao])\n",
        "\n",
        "        #filtra as similaridades por acordao e ordena por cada técnica de cálculo de similaridade\n",
        "        #a ordenacao bm25 é descendente, pois o valor é o score de similaridade do algoritmo OkapiBM25\n",
        "        #os demais também, pois são relativos à cosine similarity [0,1]\n",
        "        sim_ac_bow = similaridades[similaridades.idx_sumario.eq(index_acordao)].sort_values(['bow'], ascending=[False], ignore_index=True)\n",
        "        sim_ac_boc = similaridades[similaridades.idx_sumario.eq(index_acordao)].sort_values(['boc'], ascending=[False], ignore_index=True)\n",
        "        sim_ac_bocth = similaridades[similaridades.idx_sumario.eq(index_acordao)].sort_values(['bocth'], ascending=[False], ignore_index=True)\n",
        "        sim_ac_tfidf = similaridades[similaridades.idx_sumario.eq(index_acordao)].sort_values(['tfidf'], ascending=[False], ignore_index=True)\n",
        "        sim_ac_tfidfbocth = similaridades[similaridades.idx_sumario.eq(index_acordao)].sort_values(['tfidfbocth'], ascending=[False], ignore_index=True)\n",
        "        sim_ac_bm25 = similaridades[similaridades.idx_sumario.eq(index_acordao)].sort_values(['bm25'], ascending=[False], ignore_index=True)\n",
        "\n",
        "        #verifica posicao da similaridade para cada enunciado\n",
        "        #dados[index_acordao,3:] = enunciados associados ao acordao (a partir da 4ª coluna)\n",
        "        for index_enunciado in dados[index_acordao,3:]:\n",
        "\n",
        "            if (index_enunciado == 0):\n",
        "                break\n",
        "\n",
        "            mapa[-1].append([index_enunciado])\n",
        "\n",
        "            #localiza posicao do enunciado nas similaridades do acordao\n",
        "            pos_bow = sim_ac_bow[sim_ac_bow.idx_enunciado.eq(index_enunciado)].index.values[0]\n",
        "            pos_boc = sim_ac_boc[sim_ac_boc.idx_enunciado.eq(index_enunciado)].index.values[0]\n",
        "            pos_bocth = sim_ac_bocth[sim_ac_bocth.idx_enunciado.eq(index_enunciado)].index.values[0]\n",
        "            pos_tfidf = sim_ac_tfidf[sim_ac_tfidf.idx_enunciado.eq(index_enunciado)].index.values[0]\n",
        "            pos_tfidfbocth = sim_ac_tfidfbocth[sim_ac_tfidfbocth.idx_enunciado.eq(index_enunciado)].index.values[0]\n",
        "            pos_bm25 = sim_ac_bm25[sim_ac_bm25.idx_enunciado.eq(index_enunciado)].index.values[0]\n",
        "\n",
        "            mapa[-1][-1].append(['bow',pos_bow])\n",
        "            mapa[-1][-1].append(['boc',pos_boc])\n",
        "            mapa[-1][-1].append(['bocth',pos_bocth])\n",
        "            mapa[-1][-1].append(['tfidf',pos_tfidf])\n",
        "            mapa[-1][-1].append(['tfidfbocth',pos_tfidfbocth])\n",
        "            mapa[-1][-1].append(['bm25',pos_bm25])\n",
        "\n",
        "        #end for index_enunciado...\n",
        "\n",
        "        #cálculo do mAP\n",
        "        #bow\n",
        "        recallatk_bow = recallAtK(mapa[-1][1:], k, 1)\n",
        "\n",
        "        #boc\n",
        "        recallatk_boc = recallAtK(mapa[-1][1:], k, 2)\n",
        "\n",
        "        #bocth\n",
        "        recallatk_bocth = recallAtK(mapa[-1][1:], k, 3)\n",
        "\n",
        "        #tfidf\n",
        "        recallatk_tfidf = recallAtK(mapa[-1][1:], k, 4)\n",
        "\n",
        "        #tfidfbocth\n",
        "        recallatk_tfidfbocth = recallAtK(mapa[-1][1:], k, 5)\n",
        "\n",
        "        #bm25\n",
        "        recallatk_bm25 = recallAtK(mapa[-1][1:], k, 6)\n",
        "\n",
        "        #adiciona mAPs na lista de resultado\n",
        "        mRecallAtK.append([index_acordao, recallatk_bow, recallatk_boc, recallatk_bocth, recallatk_tfidf, recallatk_tfidfbocth, recallatk_bm25])\n",
        "\n",
        "    #end for index_acordao...\n",
        "\n",
        "\n",
        "    #retorna dataframe\n",
        "    return DataFrame(data = mRecallAtK, columns=['idx_acordao', 'recallatk_bow', 'recallatk_boc', 'recallatk_bocth', 'recallatk_tfidf', 'recallatk_tfidfbocth', 'recallatk_bm25'])\n",
        ""
      ],
      "metadata": {
        "id": "upIRHNbcQSdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94a84f9f"
      },
      "outputs": [],
      "source": [
        "#geracao de word2vec do thesaurus a partir do w2v do corpus\n",
        "def w2vThesaurusByCorpus(thesaurus_words, w2vCorpus):\n",
        "    #lista temporaria\n",
        "    thw2v = []\n",
        "    #percorre palavras do thesaurus, recuperando o w2v de cada uma delas\n",
        "    for i in range(len(thesaurus_words)):\n",
        "        try:\n",
        "            #recupera indice do palavra do thesaurus nos vetores do w2v do corpus\n",
        "            index = w2vCorpus.index_to_key.index(thesaurus_words[i])\n",
        "            #adiciona w2v da palavra do thesaurus na lista temporaria\n",
        "            thw2v.append(w2vCorpus.vectors[index])\n",
        "        except ValueError:\n",
        "            #caso a palavra do thesaurus nao esteja presente no corpus, o indice nao sera localizado e ocorrera um erro\n",
        "            #assim, sera adicionado um vetor de zeros, com a mesma dimensao dos vetores do w2v\n",
        "            thw2v.append(np.zeros_like(w2vCorpus.vectors[0]))\n",
        "\n",
        "    #retorna numpy.array\n",
        "    return np.asarray(thw2v)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "159afcad"
      },
      "outputs": [],
      "source": [
        "#geracao de word2vec a partir de outro word2vec\n",
        "#vocab: lista de palavras para obtenção dos respectivos embeddings em w2v\n",
        "#w2v: word2vec com os embeddings a serem recuperados para cada palavra em vocab\n",
        "#\n",
        "#returns vetor word2vec\n",
        "def generateW2vByVocab(vocab, w2v):\n",
        "    #lista temporaria\n",
        "    w2v_temp = []\n",
        "    #percorre palavras do vocabulario, recuperando o w2v de cada uma delas\n",
        "    for word in vocab:\n",
        "        try:\n",
        "            #adiciona w2v da palavra na lista temporaria\n",
        "            w2v_temp.append(w2v[word])\n",
        "        except KeyError:\n",
        "            #caso a palavra não exista em w2v, o indice nao sera localizado e ocorrera um erro\n",
        "            #assim, sera adicionado um vetor de zeros, com a mesma dimensao dos vetores do w2v\n",
        "            w2v_temp.append(np.zeros_like(w2v.vectors[0]))\n",
        "\n",
        "    #retorna numpy.array\n",
        "    return np.asarray(w2v_temp)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f5f9927"
      },
      "source": [
        "### Execução dos testes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asYIILGi6ir3",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "tc = TimeCounter()\n",
        "\n",
        "#configuraçao default para word2vec\n",
        "embedding_dim = 300\n",
        "context = 5\n",
        "min_freq = 1\n",
        "iterations = 5\n",
        "concepts = 300\n",
        "\n",
        "#quantidade de execuções por setup\n",
        "NUMBER_OF_EXECUTIONS = 1\n",
        "\n",
        "\n",
        "boc_model = None\n",
        "\n",
        "runningList = []\n",
        "\n",
        "runningList.append({'desc':'D300C300-SKM' ,'embedding_dim':300, 'concepts': 300, 'clustermethod': 'skm', 'normalize': True})\n",
        "runningList.append({'desc':'D300C300-KM' ,'embedding_dim':300, 'concepts': 300, 'clustermethod': 'km', 'normalize': True})\n",
        "\n",
        "runningList.append({'desc':'D400C300-SKM' ,'embedding_dim':400, 'concepts': 300, 'clustermethod': 'skm', 'normalize': True})\n",
        "runningList.append({'desc':'D400C300-KM' ,'embedding_dim':400, 'concepts': 300, 'clustermethod': 'km', 'normalize': True})\n",
        "\n",
        "runningList.append({'desc':'D500C300-SKM' ,'embedding_dim':500, 'concepts': 300, 'clustermethod': 'skm', 'normalize': True})\n",
        "runningList.append({'desc':'D500C300-KM' ,'embedding_dim':500, 'concepts': 300, 'clustermethod': 'km', 'normalize': True})\n",
        "\n",
        "runningList.append({'desc':'D300C400-SKM' ,'embedding_dim':300, 'concepts': 400, 'clustermethod': 'skm', 'normalize': True})\n",
        "runningList.append({'desc':'D300C400-KM' ,'embedding_dim':300, 'concepts': 400, 'clustermethod': 'km', 'normalize': True})\n",
        "\n",
        "runningList.append({'desc':'D400C400-SKM' ,'embedding_dim':400, 'concepts': 400, 'clustermethod': 'skm', 'normalize': True})\n",
        "runningList.append({'desc':'D400C400-KM' ,'embedding_dim':400, 'concepts': 400, 'clustermethod': 'km', 'normalize': True})\n",
        "\n",
        "runningList.append({'desc':'D500C400-SKM' ,'embedding_dim':500, 'concepts': 400, 'clustermethod': 'skm', 'normalize': True})\n",
        "runningList.append({'desc':'D500C400-KM' ,'embedding_dim':500, 'concepts': 400, 'clustermethod': 'km', 'normalize': True})\n",
        "\n",
        "#list de dictionary com o resultado das execuções\n",
        "resultados = []\n",
        "\n",
        "#vetorizacao TF-IDF considerando todo o corpus (jurisprudencia + acordaos)\n",
        "#calculado fora do loop pois é a mesma vetorização sempre, independentemente da dimensão do word2vec e dos conceitos\n",
        "t = time.time()\n",
        "print('Gerando vetorização TF-IDF *...', end = '\\r')\n",
        "vectorizerTfIdf = TfidfVectorizer(lowercase=True)\n",
        "#vetorizacao com a base de jurisprudencia original para que qualquer enunciado existente na base de\n",
        "#amostras de jurisprudencia seja localizada\n",
        "vectorsTfIdf = vectorizerTfIdf.fit_transform(pd.concat([jurisOriginal.enunciadop, acordaos.sumariop], axis=0).astype(str))\n",
        "print('Gerando vetorização TF-IDF *... Concluído.', time.time()-t)\n",
        "\n",
        "#gera amostra de jusrisprudência, que será utilizada no cálculo de similaridade para todos os setup,\n",
        "#exceto se NUMBER_OF_EXECUTIONS for maior que 1 (verificação feita mais abaixo, dentro do loop de execuções)\n",
        "juris = seleciona_amostras_jurisprudencia()\n",
        "\n",
        "for params in runningList:\n",
        "\n",
        "    print()\n",
        "    print(params['desc'])\n",
        "    tc.start(params['desc'])\n",
        "\n",
        "    #gera w2v da jurisprudencia (e do acordao), para ter o vocabulario completo\n",
        "    print('Gerando word2vec da Jurisprudência + acórdãos...')\n",
        "    tc.start('w2v juris')\n",
        "    wv_juris = Word2Vec(sentences=list(corpusJuris)+list(corpusAcordao), vector_size=params['embedding_dim'], window=context,\n",
        "                        min_count=min_freq, epochs=iterations, sg=1).wv\n",
        "\n",
        "    #print('Gerando wv_juris a partir de glove_s300... Concluído. wv_juris.shape:', wv_juris.shape)\n",
        "    tc.stop('w2v juris')\n",
        "    print('Gerando word2vec da Jurisprudência + acórdãos... Concluído.' , tc.getTime('w2v juris'))\n",
        "\n",
        "    #word2vec do VCE a partir do w2v do corpus\n",
        "    tc.start('w2v thesaurus')\n",
        "    print('Gerando word2vec do thesaurus a partir do corpus...')\n",
        "    wv_thesaurus_corpus = generateW2vByVocab(thesaurus_words_lemma, wv_juris)\n",
        "    print('wv_thesaurus_corpus.shape', wv_thesaurus_corpus.shape)\n",
        "    tc.stop('w2v thesaurus')\n",
        "    print('Gerando word2vec do thesaurus a partir do corpus... Concluído.', tc.getTime('w2v thesaurus'))\n",
        "\n",
        "\n",
        "    #modelo bag-of-concepts\n",
        "    print('Gerando BOCModel2...', end = '\\r')\n",
        "    tc.start('bocmodel')\n",
        "    boc_model = BOCModel2(corpusJuris, wv_juris.vectors, wv_juris.index_to_key,\n",
        "                          num_concept=params['concepts'], num_embedding_dim=params['embedding_dim'],\n",
        "                          num_context=context, clusterMethod=params['clustermethod'], loadCluster=False)\n",
        "    boc_matrix, word2concept_list, idx2word_converter = boc_model.fit()\n",
        "    tc.stop('bocmodel')\n",
        "    print('Gerando BOCModel2... Concluído.', tc.getTime('bocmodel'))\n",
        "\n",
        "\n",
        "    #predição de conceitos\n",
        "    print('Gerando conceptCluster...', end = '\\r')\n",
        "    concept = conceptCluster(boc_model.getClusterClass(), wv_juris, dict(word2concept_list))\n",
        "    #gera grafo de conceitos\n",
        "    concept.generateConceptGraph()\n",
        "    print('Gerando conceptCluster... Concluído.')\n",
        "\n",
        "    #classe para geração dos vetores ponderados\n",
        "    wvectors = WeigthedVectors(concept, wv_thesaurus_corpus, thesaurus_words_lemma)\n",
        "\n",
        "    #vetores ponderados da jurisprudencia\n",
        "    print('Gerando vetores ponderados da Jurisprudência...')\n",
        "    tc.start('wvecjuris')\n",
        "    wvec_juris = wvectors.get_weighted_vectors(corpusJuris)\n",
        "    tc.stop('wvecjuris')\n",
        "    print('Gerando vetores ponderados da Jurisprudência... Concluído.', tc.getTime('wvecjuris'))\n",
        "\n",
        "    #vetores ponderados dos acordaos\n",
        "    print('Gerando vetores ponderados dos Acórdãos...')\n",
        "    tc.start('wvecacordaos')\n",
        "    wvec_acordaos = wvectors.get_weighted_vectors(corpusAcordao)\n",
        "    tc.stop('wvecacordaos')\n",
        "    print('Gerando vetores ponderados dos Acórdãos... Concluído.', tc.getTime('wvecacordaos'))\n",
        "\n",
        "    #normaliza vetores bocth\n",
        "    vtemp = np.concatenate((wvec_juris,wvec_acordaos))\n",
        "    wvec_juris = wvec_juris/np.max(vtemp)\n",
        "    wvec_acordaos = wvec_acordaos/np.max(vtemp)\n",
        "\n",
        "\n",
        "    #mAP\n",
        "    mapbowmeanfull = []\n",
        "    mapbowstdfull = []\n",
        "    mapbocmeanfull = []\n",
        "    mapbocstdfull = []\n",
        "    mapbocthmeanfull = []\n",
        "    mapbocthstdfull = []\n",
        "    maptfidfmeanfull = []\n",
        "    maptfidfstdfull = []\n",
        "    maptfidfbocthmeanfull = []\n",
        "    maptfidfbocthstdfull = []\n",
        "    mapbm25meanfull = []\n",
        "    mapbm25stdfull = []\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #recall@k\n",
        "    recallatkbowmeanfull = []\n",
        "    recallatkbowstdfull = []\n",
        "    recallatkbocmeanfull = []\n",
        "    recallatkbocstdfull = []\n",
        "    recallatkbocthmeanfull = []\n",
        "    recallatkbocthstdfull = []\n",
        "    recallatktfidfmeanfull = []\n",
        "    recallatktfidfstdfull = []\n",
        "    recallatktfidfbocthmeanfull = []\n",
        "    recallatktfidfbocthstdfull = []\n",
        "    recallatkbm25meanfull = []\n",
        "    recallatkbm25stdfull = []\n",
        "\n",
        "\n",
        "    #geração do tfidf-bocth\n",
        "    #aplicando redução de dimensionalidade no vetor TF-IDF\n",
        "    t = time.time()\n",
        "    print('Redução de dimensionalidade no vetor TF-IDF a partir do BOC-Th *...')\n",
        "    tfidf_vocab = vectorizerTfIdf.get_feature_names_out()\n",
        "    vectorsTfIdf_bocth = redim_tfidf_by_concept(vectorsTfIdf, tfidf_vocab,\n",
        "                                                boc_model.num_concept, boc_model.getDictWord2Concept())\n",
        "    print('Redução de dimensionalidade no vetor TF-IDF a partir do BOC-Th *... Concluído.', time.time()-t)\n",
        "    print('vectorsTfIdf.shape *', vectorsTfIdf.shape)\n",
        "    print('vectorsTfIdf_bocth.shape *', vectorsTfIdf_bocth.shape)\n",
        "\n",
        "    #executa NUMBER_OF_EXECUTIONS o ciclo completo de calculo de similaridades\n",
        "    for execution in range(1, NUMBER_OF_EXECUTIONS+1):\n",
        "\n",
        "      #gera nova amostra de jurisprudencia somente se o numero de execuções for maior que 1\n",
        "      #caso contrário, mantém a mesma amostra para todos os setups de dimensão word2vec e conceitos\n",
        "      if (NUMBER_OF_EXECUTIONS > 1):\n",
        "          juris = seleciona_amostras_jurisprudencia()\n",
        "\n",
        "      #calcula similaridade usando Boc-Th\n",
        "      print()\n",
        "      print('*********************************')\n",
        "      print('Cálculo da similaridade', params['desc'] + '-' + str(execution) + '...')\n",
        "      tc.start('similaridade')\n",
        "      dados_bocTh = similarityBocThesaurosFull(boc_model, acordaos, juris, jurisOriginal, wvec_acordaos, wvec_juris) #, vectorsTfIdf, vectorsTfIdf_bocth)\n",
        "      tc.stop('similaridade')\n",
        "      print('Cálculo da similaridade concluído.', tc.getTime('similaridade'))\n",
        "\n",
        "      #cria dataframe e grava resultado\n",
        "      print('Geração de dataframe...', end = '\\r')\n",
        "      similaridades =  DataFrame(data = dados_bocTh, columns=['idx_sumario','sumario','idx_enunciado','enunciado',\n",
        "                                                    'bow', 'boc', 'bocth', 'tfidf', 'tfidfbocth', 'bm25'])\n",
        "\n",
        "      #ajuste dos tipos das colunas pois o array recebido foi concatenado com numpy.concatenate, o que transforma todos os\n",
        "      #tipos de colunas em um unico, no caso, string\n",
        "      convert_dict = {'idx_sumario': int,\n",
        "                      'sumario': int,\n",
        "                      'idx_enunciado': int,\n",
        "                      'enunciado': int,\n",
        "                      'bow': float,\n",
        "                      'boc': float,\n",
        "                      'bocth': float,\n",
        "                      'tfidf': float,\n",
        "                      'tfidfbocth': float,\n",
        "                      'bm25': float}\n",
        "      similaridades = similaridades.astype(convert_dict)\n",
        "\n",
        "      print('Geração de dataframe... Concluído.')\n",
        "\n",
        "\n",
        "      tc.start('map')\n",
        "      df_map = calcula_mAPFull(similaridades)\n",
        "      tc.stop('map')\n",
        "      print('Cálculo do mAP concluído.', tc.getTime('map'))\n",
        "      print()\n",
        "\n",
        "      tc.start('recallAtk')\n",
        "      df_recallatk = calcula_recallAtK(similaridades,100)\n",
        "      tc.stop('recallAtk')\n",
        "      print('Cálculo do recallAtk concluído.', tc.getTime('recallAtk'))\n",
        "\n",
        "\n",
        "\n",
        "      #mAP\n",
        "      mapbowmeanfull.append(df_map.map_bow.mean())\n",
        "      mapbowstdfull.append(df_map.map_bow.std())\n",
        "      mapbocmeanfull.append(df_map.map_boc.mean())\n",
        "      mapbocstdfull.append(df_map.map_boc.std())\n",
        "      mapbocthmeanfull.append(df_map.map_bocth.mean())\n",
        "      mapbocthstdfull.append(df_map.map_bocth.std())\n",
        "      maptfidfmeanfull.append(df_map.map_tfidf.mean())\n",
        "      maptfidfstdfull.append(df_map.map_tfidf.std())\n",
        "      maptfidfbocthmeanfull.append(df_map.map_tfidfbocth.mean())\n",
        "      maptfidfbocthstdfull.append(df_map.map_tfidfbocth.std())\n",
        "      mapbm25meanfull.append(df_map.map_bm25.mean())\n",
        "      mapbm25stdfull.append(df_map.map_bm25.std())\n",
        "\n",
        "      #recall@k\n",
        "      recallatkbowmeanfull.append(df_recallatk.recallatk_bow.mean())\n",
        "      recallatkbowstdfull.append(df_recallatk.recallatk_bow.std())\n",
        "      recallatkbocmeanfull.append(df_recallatk.recallatk_boc.mean())\n",
        "      recallatkbocstdfull.append(df_recallatk.recallatk_boc.std())\n",
        "      recallatkbocthmeanfull.append(df_recallatk.recallatk_bocth.mean())\n",
        "      recallatkbocthstdfull.append(df_recallatk.recallatk_bocth.std())\n",
        "      recallatktfidfmeanfull.append(df_recallatk.recallatk_tfidf.mean())\n",
        "      recallatktfidfstdfull.append(df_recallatk.recallatk_tfidf.std())\n",
        "      recallatktfidfbocthmeanfull.append(df_recallatk.recallatk_tfidfbocth.mean())\n",
        "      recallatktfidfbocthstdfull.append(df_recallatk.recallatk_tfidfbocth.std())\n",
        "      recallatkbm25meanfull.append(df_recallatk.recallatk_bm25.mean())\n",
        "      recallatkbm25stdfull.append(df_recallatk.recallatk_bm25.std())\n",
        "\n",
        "\n",
        "      if (NUMBER_OF_EXECUTIONS > 1):\n",
        "        print('*********************************')\n",
        "        print(params['desc'] + '-' + str(execution))\n",
        "        print('mAP bow:', mapbowmeanfull[-1],'±', mapbowstdfull[-1])\n",
        "        print('mAP boc:', mapbocmeanfull[-1],'±', mapbocstdfull[-1])\n",
        "        print('mAP bocth:', mapbocthmeanfull[-1],'±', mapbocthstdfull[-1])\n",
        "        print('mAP tfidf:', maptfidfmeanfull[-1],'±', maptfidfstdfull[-1])\n",
        "        print('mAP tfidfbocth:', maptfidfbocthmeanfull[-1],'±', maptfidfbocthstdfull[-1])\n",
        "        print('mAP bm25:', mapbm25meanfull[-1],'±', mapbm25stdfull[-1])\n",
        "        print()\n",
        "        print('recall@k bow:', recallatkbowmeanfull[-1],'±', recallatkbowstdfull[-1])\n",
        "        print('recall@k boc:', recallatkbocmeanfull[-1],'±', recallatkbocstdfull[-1])\n",
        "        print('recall@k bocth:', recallatkbocthmeanfull[-1],'±', recallatkbocthstdfull[-1])\n",
        "        print('recall@k tfidf:', recallatktfidfmeanfull[-1],'±', recallatktfidfstdfull[-1])\n",
        "        print('recall@k tfidfbocth:', recallatktfidfbocthmeanfull[-1],'±', recallatktfidfbocthstdfull[-1])\n",
        "        print('recall@k bm25:', recallatkbm25meanfull[-1],'±', recallatkbm25stdfull[-1])\n",
        "\n",
        "\n",
        "      # end for execution...\n",
        "\n",
        "\n",
        "    resultados.append({'desc': params['desc'],\n",
        "                     'mapbowmean':np.mean(mapbowmeanfull), 'mapbowstd':np.mean(mapbowstdfull),\n",
        "                     'mapbocmean':np.mean(mapbocmeanfull), 'mapbocstd':np.mean(mapbocstdfull),\n",
        "                     'mapbocthmean':np.mean(mapbocthmeanfull), 'mapbocthstd':np.mean(mapbocthstdfull),\n",
        "                     'maptfidfmean':np.mean(maptfidfmeanfull), 'maptfidfstd':np.mean(maptfidfstdfull),\n",
        "                     'maptfidfbocthmean':np.mean(maptfidfbocthmeanfull), 'maptfidfbocthstd':np.mean(maptfidfbocthstdfull),\n",
        "                     'mapbm25mean':np.mean(mapbm25meanfull), 'mapbm25std':np.mean(mapbm25stdfull),\n",
        "\n",
        "                     'recallatkbowmean':np.mean(recallatkbowmeanfull), 'recallatkbowstd':np.mean(recallatkbowstdfull),\n",
        "                     'recallatkbocmean':np.mean(recallatkbocmeanfull), 'recallatkbocstd':np.mean(recallatkbocstdfull),\n",
        "                     'recallatkbocthmean':np.mean(recallatkbocthmeanfull), 'recallatkbocthstd':np.mean(recallatkbocthstdfull),\n",
        "                     'recallatktfidfmean':np.mean(recallatktfidfmeanfull), 'recallatktfidfstd':np.mean(recallatktfidfstdfull),\n",
        "                     'recallatktfidfbocthmean':np.mean(recallatktfidfbocthmeanfull), 'recallatktfidfbocthstd':np.mean(recallatktfidfbocthstdfull),\n",
        "                     'recallatkbm25mean':np.mean(recallatkbm25meanfull), 'recallatkbm25std':np.mean(recallatkbm25stdfull)})\n",
        "\n",
        "    tc.stop(params['desc'])\n",
        "\n",
        "    print('*********************************')\n",
        "    r = resultados[-1] #ultimo registro na lista 'resultados'\n",
        "    print(r['desc'])\n",
        "    print('mAP bow:', r['mapbowmean'],'±', r['mapbowstd'])\n",
        "    print('mAP boc:', r['mapbocmean'],'±', r['mapbocstd'])\n",
        "    print('mAP bocth:', r['mapbocthmean'],'±', r['mapbocthstd'])\n",
        "    print('mAP tfidf:', r['maptfidfmean'],'±', r['maptfidfstd'])\n",
        "    print('mAP tfidfbocth:', r['maptfidfbocthmean'],'±', r['maptfidfbocthstd'])\n",
        "    print('mAP bm25:', r['mapbm25mean'],'±', r['mapbm25std'])\n",
        "    print()\n",
        "\n",
        "    print('recall@k bow:', r['recallatkbowmean'],'±', r['recallatkbowstd'])\n",
        "    print('recall@k boc:', r['recallatkbocmean'],'±', r['recallatkbocstd'])\n",
        "    print('recall@k bocth:', r['recallatkbocthmean'],'±', r['recallatkbocthstd'])\n",
        "    print('recall@k tfidf:', r['recallatktfidfmean'],'±', r['recallatktfidfstd'])\n",
        "    print('recall@k tfidfbocth:', r['recallatktfidfbocthmean'],'±', r['recallatktfidfbocthstd'])\n",
        "    print('recall@k bm25:', r['recallatkbm25mean'],'±', r['recallatkbm25std'])\n",
        "    print('*********************************')\n",
        "    print(params['desc'])\n",
        "    print('(tempo', tc.getTime(params['desc']), ')')\n",
        "    print('*********************************')\n",
        "    print()\n",
        "\n",
        "#end for param...\n",
        "\n",
        "print()\n",
        "print()\n",
        "for r in resultados:\n",
        "    print('*********************************')\n",
        "    print(r['desc'])\n",
        "    print('mAP bow:', r['mapbowmean'],'±', r['mapbowstd'])\n",
        "    print('mAP boc:', r['mapbocmean'],'±', r['mapbocstd'])\n",
        "    print('mAP bocth:', r['mapbocthmean'],'±', r['mapbocthstd'])\n",
        "    print('mAP tfidf:', r['maptfidfmean'],'±', r['maptfidfstd'])\n",
        "    print('mAP tfidfbocth:', r['maptfidfbocthmean'],'±', r['maptfidfbocthstd'])\n",
        "    print('mAP bm25:', r['mapbm25mean'],'±', r['mapbm25std'])\n",
        "    print()\n",
        "    print('recall@k bow:', r['recallatkbowmean'],'±', r['recallatkbowstd'])\n",
        "    print('recall@k boc:', r['recallatkbocmean'],'±', r['recallatkbocstd'])\n",
        "    print('recall@k bocth:', r['recallatkbocthmean'],'±', r['recallatkbocthstd'])\n",
        "    print('recall@k tfidf:', r['recallatktfidfmean'],'±', r['recallatktfidfstd'])\n",
        "    print('recall@k tfidfbocth:', r['recallatktfidfbocthmean'],'±', r['recallatktfidfbocthstd'])\n",
        "    print('recall@k bm25:', r['recallatkbm25mean'],'±', r['recallatkbm25std'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print()\n",
        "\n",
        "pd.DataFrame(resultados).to_excel(files_path + 'resultado_similaridadeFull.xlsx', index=False)\n",
        "print(files_path + 'resultado_similaridadeFull.xlsx gerado.')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1X9NeLA-AV0_eXYRuMnf7sdaYxsUxp-kp",
      "authorship_tag": "ABX9TyNTxerMUoZxssYVF+Q9ChS+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}